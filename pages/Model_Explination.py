import streamlit as stimport pandas as pd st.write("## Model Explanation")st.markdown('''    The model used in this project was an XGBoosted Classification model.     This choice was made because it is a supervised approach. Intrinsic to     predicting which team wins a game is understanding why they won. As a     supervised learning algorithm, XGBoost provides us with the importance     of each feature, helping us understand why a game was classified as an upset or not.        The data used in this model comes from Kaggle, which provides detailed     information on each regular season and postseason game since 2003. The     specific dataset used in this architecture includes regular season averages     for statistics like 3-pointers made and missed, turnovers, points scored,     number of regular season wins and losses, conference affiliation, average     points given up, etc.        The model takes the regular season data for two teams playing against each     other and then looks at past tournaments to predict, based on historical     NCAA Tournament games, whether the higher-seeded team will win or lose when     considering the regular season data.        The higher seed, in this case, refers to the rank order of teams 1-64 who     make the field of 64. Therefore, the user must select the play-in games to     run the model. If a team is not in the field of 64, it does not receive a     "power seed." This prevents scenarios like a 1 vs. 1 seed matchup in the     Final Four or a 2 vs. 2 matchup, etc. Each tournament team is assigned a     power seed from 1-64. The outcome variable for this project is whether the     higher "power seed" wins.    ''')    st.write("### What is a Boosted Forest Model? How does it work? ###")st.markdown('''    Forest models are a robust framework for data science, capable of handling     classification, regression, and ranking problems. They operate based on the     concept of decision trees, which are "weak learners" that iteratively learn     from each other. This ensemble of learners results in a powerful predictive model.        Gradient boosting takes these sequential decision trees and uses the residual     errors from the previous model as the targets.        Essentially, for regression, here are our steps:        1. Fit a decision tree (assuming this is our base model, which is the most common).    2. Calculate the residuals: true training values - predicted training values.        Note: This is equivalent to taking the negative gradient of the loss function,        so it can generalize to other loss functions for classification and ranking tasks.    3. Train a second decision tree where the residuals are the targets.    4. Continue this process for the specified number of estimators (hyper-parameter).        At prediction time, each tree makes a prediction, and in a classification     problem, they vote, with the majority class winning (#Democracy).        It's important to note that the learning rate for gradient boosting scales     the combination of the sequential trees.        One potential critique from a statistician is the lack of pure interpretability     compared to linear or logistic regression, where coefficients can be interpreted     directly. In essence, think of a tree model as creating a flowchart, splitting     along certain axes of the data until it reaches a group of outcomes that are     mostly of one class. It then repeats this process to reach a consensus.    ''')st.write("### Visualizations from the Project")st.markdown('''    Below is an examples of feature importance in the model, for more questions, pleas     contact me at jamesgriffd@gmail.com, find me on linkedin or if you have my     number shoot me a text :) C    ''')st.image("../viz/Feat_imp_example.png")
