import streamlit as stimport pandas as pdst.write("## Model Explanation")st.markdown('''    The model used in this project was an XGBoosted Classification model.     This choice was made because it is a supervised approach. Intrinsic to    predicting which team wins a game is understanding why they won. As a    supervised learning algorithm, XGBoost provides us with the importance    of each feature, helping us understand why a game was classified as an upset or not.    The data used in this model comes from Kaggle, which provides detailed     infrmation on each regular season and postseason game since 2003. The     specfic dataset used in this architecture includes regular season averages     for satistics like 3-pointers made and missed, turnovers, points scored,     numberof regular season wins and losses, conference affiliation, average     points iven up, etc.        The m takes the regular season data for two teams playing against each     other and thn looks at past tournaments to predict, based on historical     NCAA Tournamet games, whether the higher-seeded team will win or lose when     considering th regular season data.        The higher s in this case, refers to the rank order of teams 1-64 who     make the field of 6. Therefore, the user must select the play-in games to     run the model. If a eam is not in the field of 64, it does not receive a     "power seed." This prvents scenarios like a 1 vs. 1 seed matchup in the     Final Four or a 2 vs.  matchup, etc. Each tournament team is assigned a     power seed from 1-64. Te outcome variable for this project is whether the     higher "power seed" wins    ''')    st.write("### What is a Bod Forest Model? How does it work? ###")st.markdown('''    Forest models are a robust framework for data science, capable of handling     classification, regression, ad ranking problems. They operate based on the     concept of decision trees, whih are "weak learners" that iteratively learn     from each other. This ensemble f learners results in a powerful predictive model.        Gradient boosting takes theseuential decision trees and uses the residual     errors from the previous model as th targets.        Essentially, for regression, here our steps:        1. Fit a decision tree (assuming this ur base model, which is the most common).    2. Calculate the residuals: true training values - predicted training values.        Note: This is equivalent to taking the negtive gradient of the loss function,        so it can generalize to other loss function for classification and ranking tasks.    3. Train a second decision tree where the residuals are the targets.    4. Continue this process for the specified number of estimators (hyper-parameter).        At prediction time, each tree makes a predic, and in a classification     problem, they vote, with the majority class winning(#Democracy).        It's important to note that the learning rate fordient boosting scales     the combination of the sequential trees.        One poential critique from a statistician is the lackpure interpretability     compared to linear or logistic regression, where coefficientscan be interpreted     directly. In essence, think of a tree model as creating a flowhart, splitting     along certain axes of the data until it reaches a group of outcmes that are     mostly of one class. It then repeats this process to reach a conensus.    ''')st.write("### Visualizations from the Project")st.markdown('''    Below is an examples of feature importance in the model, for more questions, pleas     contact me at jamesgriffd@gmail.com, find me on linkedin or if yo have my     number shoot me a text :)     ''')st.image("Feat_imp_example.png"
