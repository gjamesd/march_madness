{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9b7b288-4216-4e29-b88c-7e8b327acdef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing LOTS of stuff \n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn import linear_model\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import f1_score, mean_squared_error, classification_report \n",
    "import random \n",
    "from random import sample\n",
    "\n",
    "# to solve problems that I am encounterinbg\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt #plotting for visualization purposes of story telling \n",
    "import numpy as np\n",
    "\n",
    "#from tqdm import tqdm\n",
    "\n",
    "import datetime\n",
    "import gc\n",
    "import seaborn as sns\n",
    "sns.set_palette(sns.color_palette('hls', 7))\n",
    "\n",
    "from statistics import mean\n",
    "from collections import Counter\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "from collections import Counter\n",
    "#from imblearn.over_sampling import RandomOverSampler\n",
    "import xgboost as xgb\n",
    "\n",
    "from scipy import linalg as la\n",
    "import networkx as nx\n",
    "from itertools import combinations\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from tqdm import tqdm\n",
    "from itertools import product\n",
    "\n",
    "import pickle \n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea598c9-d411-4779-bffe-f11e76d4c11b",
   "metadata": {},
   "source": [
    "# Step 0 - read in data and libraries for process \n",
    "\n",
    "for a full data description from kaggle, visit this webstie: https://www.kaggle.com/competitions/mens-march-mania-2022/data\n",
    "\n",
    "The only thing i created by hand is Power the power seeding dictionary. The selection comitte does this, selects teams seeds the tournament. The top overal seed is obviously 1, the second 1 seed, on the opposite side of the bracket is 2, on the same side of the bracket is the third overall and then the fourth overall 1 seed is on the same side as the 1st overall seed. This means that the 5th overall seed is the \"best\" 2 seed and get the \"wrost\" 1 seed. and then the process is cyclic, rotating around the tournament from 1 to 64. This is the variable in question I use to generate the outcome variable: Did the higher seed win? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f0246de-6235-4c6a-8cf4-ab9eae02d6ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#what year are we running the analysis for? \n",
    "year_ = 2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca8ab4ce-3ece-466f-98f1-89aaeecdf419",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import lots of data \n",
    "# teams that could make the tournament all D1 teams \n",
    "teams_df = pd.read_csv(\"../data/MTeams.csv\")\n",
    "\n",
    "#detailed game by game of tournament games from 2003 onward \n",
    "tourney_deets = pd.read_csv(\"../data/MNCAATourneyDetailedResults.csv\")\n",
    "\n",
    "#regular season games from 2003 onwards \n",
    "reg_deets = pd.read_csv(\"../data/MRegularSeasonDetailedResults.csv\")\n",
    "\n",
    "#winner and loser of tournament games since 1985 \n",
    "tourney_compact_deets =  pd.read_csv(\"../data/MNCAATourneyCompactResults.csv\")\n",
    "\n",
    "#historical tournament seeds \n",
    "tourney_seeds = pd.read_csv(\"../data/MNCAATourneySeeds.csv\")\n",
    "\n",
    "#conference tournament games detailed results \n",
    "conf_tour = pd.read_csv(\"../data/MConferenceTourneyGames.csv\")\n",
    "\n",
    "#what conferences had who in what year \n",
    "conf = pd.read_csv(\"../data/MTeamConferences.csv\")\n",
    "\n",
    "#hand made varialbe used to create target \n",
    "#power_seeds = pd.read_csv(\"../data/PowerSeeds.csv\")\n",
    "power_seeds_df = pd.read_csv(\"../data/PowerSeeds.csv\")\n",
    "\n",
    "compact = pd.read_csv('../data/MNCAATourneyCompactResults.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c78891-11ad-4bec-bb17-d7be46f3c9a7",
   "metadata": {},
   "source": [
    "# Step 1 Data Prep / build dictionaries for features \n",
    "\n",
    "The goal here is to use among other things regular season values for each team for each season as features for the model. We will create a multitude of different dictionaries with seasons and teams as keys and things like average 3's made, average turnovers, number of wins etc as values. This will be the general approach we take to model feature creation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89a4bce8-8f5e-4c09-bc0e-bd89c8b9f262",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Define the options for each game\n",
    "game_1 = ('Howard','Wagner')\n",
    "game_2 = ('Montana St','Grambling')\n",
    "game_3 = ('Virginia','Colorado St')\n",
    "game_4 = ('Boise St','Colorado')\n",
    "\n",
    "# Generate all possible combinations of winners for the 4 games\n",
    "possible_combinations = list(product(*[game_1, game_2, game_3, game_4]))\n",
    "\n",
    "# Print all possible combinations of winners\n",
    "combos = []\n",
    "for combination in possible_combinations:\n",
    "    combos.append(list(combination))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c188223e-2f6d-435d-8be7-ff0d2d61449c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#power_seeds[power_seeds['Season'] == 2023]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b394e39-bc89-4f02-bee6-59b8f92f448a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7666289f-6ebd-4328-9756-110890b2af87",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def put_in_first_four(df, c_list):\n",
    "    \n",
    "    \n",
    "\n",
    "    eleven_1_index = df.loc[(df['Season'] == year_)&(df['PowerSeed '] == 38)].index[0]\n",
    "    eleven_2_index = df.loc[(df['Season'] == year_)&(df['PowerSeed '] == 39)].index[0]\n",
    "\n",
    "    sixteen_1_index = df.loc[(df['Season'] == year_)&(df['PowerSeed '] == 61)].index[0]\n",
    "    sixteen_2_index = df.loc[(df['Season'] == year_)&(df['PowerSeed '] == 62)].index[0]\n",
    "\n",
    "    df['TeamName'][eleven_1_index] = c_list[2]\n",
    "    df['TeamName'][eleven_2_index] = c_list[3]\n",
    "\n",
    "    df['TeamName'][sixteen_1_index] = c_list[0]\n",
    "    df['TeamName'][sixteen_2_index] = c_list[1]\n",
    "    \n",
    "    \n",
    "    return df \n",
    "#power_seeds = put_in_first_four(power_seeds, combos[7])\n",
    "#power_seeds.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75a16d1-c200-407e-ba3a-a394525d3553",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d6180b1-51aa-4da7-8a5d-bd2f9acd7451",
   "metadata": {},
   "outputs": [],
   "source": [
    "#psd  = create_power_seeds_dict(power_seeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7c754dce-0b75-4fd6-8ce3-afffb5bcd47a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# p5 = put_in_first_four(power_seeds_df, combos[15])\n",
    "# ps5 = create_power_seeds_dict(p5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3526e5cb-b03d-49c7-8037-c0e733f11d73",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f7a438-a3be-46f0-a5d3-577a8dd0b19f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3cdf1f14-f453-4706-92c1-06ad8e79f8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_conf_affiliation():\n",
    "    '''\n",
    "    Parameters: None\n",
    "    output: regular season detailed results with the winning \n",
    "    and losing team conference afiiliation denoted \n",
    "    \n",
    "    prepping a data frame for getting conference affiliation \n",
    "    by season by year here below\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    step1 = pd.merge(left = reg_deets, right = conf_tour, on = ['Season', \"WTeamID\"]) \n",
    "    \n",
    "    step1 = step1.drop(columns = ['DayNum_y', 'LTeamID_y'])\n",
    "    step1 = step1.rename(columns = {'DayNum_x':'DayNum',\n",
    "                                   'ConfAbbrev':\"WTeam_Conf\", \n",
    "                                   \"LTeamID_x\":\"LTeamID\"})\n",
    "    \n",
    "    step2 = pd.merge(left = step1, right = conf_tour, on = ['Season', \"LTeamID\"]) \n",
    "    step2 = step2.drop(columns = ['DayNum_y', 'WTeamID_y'])\n",
    "    step2 = step2.rename(columns = {'DayNum_x':'DayNum','ConfAbbrev':\"LTeam_Conf\",\n",
    "                                   'WTeamID_x':'WTeamID'})\n",
    "    \n",
    "    return step2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c3ccf26b-0a80-4137-b09d-3db02138ee4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_winning_dict(df, column):\n",
    "    '''\n",
    "    building a dictionary for the averages by season by team for when they \n",
    "    were the winning team: \n",
    "    \n",
    "    Note - it is possible for a team to go winless, we will need to \n",
    "    figure out how to deal with that should we have an undefeated regular season--\n",
    "    although these teams will not play in march and it seams very unlikely that \n",
    "    they don't get a single win, it is possible lol \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    features = ['WScore', 'LScore', 'WFGM', 'WFGA', 'WFGM3','WFGA3', 'WFTM', 'WFTA', 'WOR',\n",
    "                'WTO',\n",
    "            'WDR', 'WAst', 'WStl', 'WBlk', 'WPF', 'LFGM', 'LFGA', 'LFGM3', 'LFGA3',\n",
    "       'LFTM', 'LFTA', 'LOR', 'LDR', 'LAst', 'LTO', 'LStl', 'LBlk', 'LPF', ]\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "    w = df.groupby(by=[column])\n",
    "    w1 = pd.DataFrame(w)\n",
    "    teams = {}\n",
    "    \n",
    "    for i in tqdm(range(len(w1))):\n",
    "        #print(i)\n",
    "        ws = w1[1][i].groupby(\"Season\")\n",
    "        ws = pd.DataFrame(ws)\n",
    "        #print(ws['Season'])\n",
    "        seasons = {}\n",
    "        for j in range(len(ws)):\n",
    "            season = ws[0][j]\n",
    "            #print(season)\n",
    "            \n",
    "            vals = dict(ws[1][j][features].sum(axis = 0))\n",
    "            vals.update({'season_wins': len(ws[1][j])})\n",
    "            #vals.update({'conf':list(set(ws[1][j]['WTeam_Conf']))[0]})\n",
    "            #print(v_dict)\n",
    "            seasons.update({ws[0][j]:  vals})\n",
    "        \n",
    "        teams.update({w1[0][i]:seasons})\n",
    "    return teams\n",
    "        \n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94f8e54-b2ba-4eb3-addb-3dc6c47be52b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1c1de851-66d3-424d-a3df-c39268a51a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_losing_dict(df, column):\n",
    "    \n",
    "    '''\n",
    "    building a dictionary for the averages by season by team for when they \n",
    "    were the losing team: \n",
    "    \n",
    "    Note - it is possible for a team to go undefeated, we will need to \n",
    "    figure out how to deal with that should we have an undefeated regular season\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    \n",
    "    features =['WScore', 'LScore', 'WFGM', 'WFGA', 'WFGM3','WFGA3', 'WFTM', 'WFTA', 'WOR',\n",
    "                'WTO',\n",
    "            'WDR', 'WAst', 'WStl', 'WBlk', 'WPF', 'LFGM', 'LFGA', 'LFGM3', 'LFGA3',\n",
    "       'LFTM', 'LFTA', 'LOR', 'LDR', 'LAst', 'LTO', 'LStl', 'LBlk', 'LPF', ]\n",
    "    \n",
    "    \n",
    "    rename = ['LScore', 'WScore', 'LFGM', 'LFGA', 'LFGM3','LFGA3', 'LFTM', 'LFTA', 'LOR',\n",
    "                'LTO',\n",
    "            'LDR', 'LAst', 'LStl', 'LBlk', 'LPF', 'WFGM', 'WFGA', 'WFGM3', 'WFGA3',\n",
    "       'WFTM', 'WFTA', 'WOR', 'WDR', 'WAst', 'WTO', 'WStl', 'WBlk', 'WPF', ]\n",
    "    \n",
    "        \n",
    "    w = df.groupby(by=[column])\n",
    "    w1 = pd.DataFrame(w)\n",
    "    teams = {}\n",
    "    \n",
    "    for i in tqdm(range(len(w1))):\n",
    "        #print(i)\n",
    "        ws = w1[1][i].groupby(\"Season\")\n",
    "        ws = pd.DataFrame(ws)\n",
    "        seasons = {}\n",
    "        for j in range(len(ws)):\n",
    "            season = ws[0][j]\n",
    "            data = ws[1][j][features]\n",
    "            data.columns = rename\n",
    "            #vals = dict(data.sum(axis = 0)/len(data))\n",
    "            vals = dict(data.sum(axis = 0))\n",
    "            vals.update({'season_losses': len(ws[1][j])})\n",
    "\n",
    "            seasons.update({ws[0][j]:  vals})\n",
    "        \n",
    "        teams.update({w1[0][i]:seasons})\n",
    "    return teams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e19ca6-7109-4227-ab9c-118cd82cdacd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "37bbe3ac-87b5-4468-875d-ce2520b83675",
   "metadata": {},
   "source": [
    "#### Offensive Efficiency\n",
    "\n",
    "Calculate the number of total number of possessions for your team using the formula: field goals attempted - offensive rebounds + turnovers + (0.4 x free throws attempted) = total number of possessions for the season. This works because a possession can end only in one of three ways: an attempted field goal, a turnover or a free throw, with an offensive rebound negating additional field goal attempts.\n",
    "\n",
    "Divide the team's total points scored for the season by the possessions you calculated in Step 1. For example, 938 total points scored divided by 998 total possessions gives your team 0.94 points scored per possession. Numbers above 1.0 are generally considered good.\n",
    "\n",
    "Convert the offensive PPP number to an efficiency rating by simply multipling by 100. So 0.94 points scored per possession becomes an offensive efficiency rating of 94.\n",
    "\n",
    "#### Defensive Efficiency\n",
    "\n",
    "Use the formula field goals attempted - offensive rebounds + turnovers + (0.4 x free throws attempted) = total number of possessions for the season to calculate total team possessions.\n",
    "\n",
    "Divide the total number of points allowed by your team by the possession total you calculated in Step 1. For example, 1009 total points allowed divided by 998 total possessions gives your team 1.01 points allowed per possession. The opposite is true for defensive PPP: Above 1.0 is bad; below 1.0 is considered good.\n",
    "\n",
    "Convert defensive points per possession to a defensive efficiency rating by multiplying by 100. So 1.01 points allowed per possession becomes a defensive efficiency rating of 101.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3b2981-89f4-48e9-8dc4-acae9ddd4259",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1fd46f41-8033-4444-b346-7c34870ab020",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine2(w_dict, l_dict):\n",
    "    '''\n",
    "    Parameters: winnign dictioanry, losing dictionary \n",
    "    \n",
    "    combining the dictioanries for games won and games lost \n",
    "    for the regular season for teams. we will then combine them \n",
    "    and get averages for each statistical category and compute \n",
    "    things like offensive and defensive efficiency. \n",
    "    \n",
    "    -- NOTE: if we use one of the efficiency metrics, we will not\n",
    "    be able to use the features that are inputs to that column. \n",
    "    because as given, the efficiency is simply a linear combination\n",
    "    of other features, this will make our matrix non-invertable as our \n",
    "    columns will not be independent, so we will have to choose to either \n",
    "    keep all of the features seperate or combine and drop some \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    \n",
    "    total = {}\n",
    "    first = reg_deets.WTeamID.min()\n",
    "    last = reg_deets.WTeamID.max()+1\n",
    "    \n",
    "    features = ['WScore', 'LScore', 'WFGM', 'WFGA', 'WFGM3','WFGA3', 'WFTM', 'WFTA', 'WOR',\n",
    "                'WTO',\n",
    "            'WDR', 'WAst', 'WStl', 'WBlk', 'WPF', 'LFGM', 'LFGA', 'LFGM3', 'LFGA3',\n",
    "       'LFTM', 'LFTA', 'LOR', 'LDR', 'LAst', 'LTO', 'LStl', 'LBlk', 'LPF',]\n",
    "    \n",
    "    #for each team from the frist teamID to the last one \n",
    "    for i in tqdm(range(first, last+1)):\n",
    "    #for i in range(first, last+1):\n",
    "        #print(i)\n",
    "            \n",
    "        season = {}\n",
    "        f_season = 2003 #pd.DataFrame(w_dict[1102]).T.index.min()\n",
    "        l_season = year_ #pd.DataFrame(w_dict[1102]).T.index.max()+1\n",
    "        w_l = {}\n",
    "        for j in range(f_season, l_season+1):\n",
    "            #print(j)\n",
    "            try:\n",
    "                w_df = pd.DataFrame(w_dict[i]).T\n",
    "                l_df = pd.DataFrame(l_dict[i]).T\n",
    "\n",
    "                w_df_r = w_df.reset_index()\n",
    "                l_df_r = l_df.reset_index()\n",
    "\n",
    "\n",
    "\n",
    "                temp  = pd.concat([w_df_r[w_df_r['index'] == j],l_df_r[l_df_r['index'] == j]])\n",
    "                temp = temp.fillna(0)\n",
    "\n",
    "                games = np.sum(temp['season_losses'])+np.sum(temp['season_wins'])\n",
    "\n",
    "                #temp[features].sum(axis = 0)/games\n",
    "                season_avg = dict(temp[features].sum(axis = 0)/games)\n",
    "\n",
    "                w_l.update({'num_loss': np.sum(temp['season_losses'])})\n",
    "                w_l.update({'num_wins': np.sum(temp['season_wins'])})\n",
    "\n",
    "                season_avg = season_avg|w_l\n",
    "\n",
    "\n",
    "                new_key = ['Score', 'Op_score', 'FGM', 'FGA', 'FGM3', 'FGA3', 'FTM', 'FTA', 'OR', 'TO', \n",
    "               'DR', 'Ast', 'Stl', 'Blk', 'PF', 'Op_FGM', 'Op_FGA', 'Op_FGM3', 'Op_FGA3', \n",
    "               'Op_FTM', 'Op_FTA', 'Op_OR', 'Op_DR', 'Op_Ast', 'Op_TO', 'Op_Stl', \n",
    "               'Op_Blk', 'Op_PF', 'num_loss', 'num_wins', 'conf']\n",
    "\n",
    "                vals = list(season_avg.values())\n",
    "                #print(vals)\n",
    "                new = dict(zip(new_key, vals))\n",
    "                off_eff = (new['FGA'] - new['OR']+new['Op_TO']+(0.47*new['FTA']))/(new['Score'])*100\n",
    "                new.update({'off_eff':off_eff})\n",
    "\n",
    "                def_eff = (new['Op_FGA'] - new['Op_OR']+new['TO']+(0.47*new['Op_FTA']))/(new['Op_score'])*100\n",
    "                new.update({'def_eff':def_eff})\n",
    "\n",
    "                equalizer = (new['FGM3']/new['FGA3'])*100\n",
    "                new.update({'three_perc':equalizer})\n",
    "\n",
    "                d_equal = (new['Op_FGM3']/new['Op_FGA3'])*100\n",
    "                new.update({'d_three_perc':d_equal})\n",
    "\n",
    "                #print(vals)\n",
    "                #season.update({j:  season_avg})\n",
    "                season.update({j:  new})\n",
    "\n",
    "\n",
    "\n",
    "            except:\n",
    "                #print(i)\n",
    "                pass\n",
    "    \n",
    "        total.update({i:season})\n",
    "    return total "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4db5eb-5e2e-453d-86d5-e799035c93f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "94461a5d-1e15-49fc-b187-d0b18a4f472f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_conf_dict(df):\n",
    "    \n",
    "    '''\n",
    "    Many of the tournament titles are played on \"Selection Sunday\"\n",
    "    the day the tournament is seeded.  others are played throughout\n",
    "    \"Championship Week\". We will need to find a way to group by \n",
    "    conference and select the winner on the last day a game was played \n",
    "    for a particular conference \n",
    "\n",
    "    '''\n",
    "    \n",
    "    conf_champ = conf_tour.groupby([\"Season\", \"ConfAbbrev\"])[\"DayNum\"].max() \n",
    "    \n",
    "    cdf = pd.DataFrame(conf_champ).reset_index()\n",
    "    cdf2 = pd.merge(left = cdf, right = conf_tour, on = ['Season', 'ConfAbbrev', 'DayNum'])\n",
    "    cdf2 = cdf2.drop(columns = ['LTeamID'])\n",
    "    \n",
    "    teams = {}\n",
    "    cdf2 = pd.DataFrame(cdf2.groupby('Season'))\n",
    "    for i in tqdm(range(len(cdf2))):\n",
    "        #print(i)\n",
    "        season = cdf2[1][i]\n",
    "        year = cdf2[0][i]\n",
    "        champs = dict(zip(cdf2[1][i]['WTeamID'],cdf2[1][i]['ConfAbbrev']))\n",
    "        teams.update({year: champs})\n",
    "\n",
    "    return teams, cdf2\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72fa715b-6c24-46bb-9569-6a1abd3f9633",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9919b114-5d64-4488-ad24-34d8c4022976",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_conf_affiliation_dict(df):\n",
    "    '''\n",
    "    Parameters: Regular deets 2 which has been merged with the \n",
    "    conference tournament data to get the conference afiiliatoin \n",
    "    by season by team\n",
    "    As teams can  move conferences from year to year, it is unclear \n",
    "    if the marginal change affects teams, but we wanted to gather the \n",
    "    confernece affect and then group them into power, mid major and \n",
    "    low major levels to be done later \n",
    "    \n",
    "    \n",
    "    '''\n",
    "\n",
    "    t1 = pd.DataFrame(df.groupby(\"Season\"))\n",
    "    cf = {}\n",
    "    for i in range(len(t1)):\n",
    "        \n",
    "        d1 = dict(zip(t1[1][i]['WTeamID'], t1[1][i]['WTeam_Conf']))\n",
    "        d2 = dict(zip(t1[1][i]['LTeamID'], t1[1][i]['LTeam_Conf']))\n",
    "        d3 =d1|d2\n",
    "        cf.update({t1[0][i]:d3})\n",
    "    \n",
    "        \n",
    "    return cf\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57686614-b037-49b8-95f0-fa633dc0c1cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "86ab1b57-f01a-48df-b60e-5fa7e6ca9683",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_power_seeds_dict(df):\n",
    "    \n",
    "    #first add team id to TeamNamein power seeds \n",
    "    \n",
    "    teams2 = teams_df[['TeamID', 'TeamName']]\n",
    "    \n",
    "    df2 = pd.merge(left = df, right = teams2, on = 'TeamName') \n",
    "    \n",
    "    df2.to_csv('../data/power_test.csv')\n",
    "    ps1 = pd.DataFrame(df2.groupby(by = \"Season\"))\n",
    "    empty = {}\n",
    "    #print(ps1.columns.tolist())\n",
    "    # for i in range(len(ps1)):\n",
    "    #     empty.update({ps1[0][i]: dict(zip(ps1[1][i]['TeamID'], ps1[1][i]['PowerSeed ']))})\n",
    "    result_dict = {}\n",
    "    for season, season_group in df2.groupby('Season'):\n",
    "        season_dict = {}\n",
    "        for index, row in season_group.iterrows():\n",
    "            team_name = row['TeamID']\n",
    "            power_seed = row['PowerSeed ']\n",
    "            season_dict[team_name] = power_seed\n",
    "        result_dict[season] = season_dict\n",
    "\n",
    "\n",
    "    return result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f18c88f-8c7d-4116-9e73-fc8bff2a04ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70edf967-fad2-463b-a13c-0168bc7f7c40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0d37922e-fd32-4ae5-9921-fb64045e75b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_power_seeds_tourney(df, psd):\n",
    "    ''' takes in tourney_deets and gets L team and w team power seeds\n",
    "    \n",
    "    TIP: 134, 135 are play in days \n",
    "    '''\n",
    "    \n",
    "    \n",
    "    df = df[df['DayNum']  >= 137]\n",
    "    df = df.reset_index()\n",
    "    df = df.drop(columns =['index'])\n",
    "    \n",
    "    lteam = []\n",
    "    wteam = []\n",
    "    for i in tqdm(range(len(df))):\n",
    "        #print(i)\n",
    "        try: \n",
    "            #print('inside try') \n",
    "            lteam.append(psd[df['Season'][i]][df['LTeamID'][i]])\n",
    "            wteam.append(psd[df['Season'][i]][df['WTeamID'][i]])\n",
    "            #pass\n",
    "        except:  \n",
    "            print(df['Season'][i])\n",
    "            print(i)\n",
    "            print(df['LTeamID'][i])\n",
    "            print(\" \")\n",
    "            #wteam.append(\"error\")\n",
    "            #lteam.append('error')\n",
    "            #pass\n",
    "    df['lteam_seed'] = lteam\n",
    "    df['wteam_seed'] = wteam\n",
    "    \n",
    "    df = df.reset_index()\n",
    "    df = df.drop(columns = ['index'])\n",
    "    return df \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8298eda-0684-4b1c-ad10-e2de677e5e93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "969a8f98-8209-467d-bb5f-45c5b61c05e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2fa20aba-d973-4136-86d1-ed6bb1bfbf3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_data(df):\n",
    "    df = df.reset_index()\n",
    "    ''' \n",
    "    takes in tourney_deets df and \n",
    "    renames columns based on seeding of low and high seeds\n",
    "    instead of winning and losing teams, this way our \n",
    "    model will not just memorize the order of columns and that \n",
    "    will tell the computer who won. And the games we have haven't \n",
    "    happened yet so like it doesnt even make sense to run that way lol \n",
    "    \n",
    "    \n",
    "    ''' \n",
    "    \n",
    "    fav = {'Season':'Season', 'DayNum':'DayNum',\n",
    "           \n",
    "           'WTeamID':'HSTeamID', \n",
    "           'LTeamID':'LSTeamID', 'WLoc':'WLoc', 'NumOT':'NumOT', \n",
    "       \n",
    "       'WFGM':'HSFGM', 'WFGA':'HSFGA', 'WFGM3':'HSFGM3', \n",
    "       'WFGA3':'HSFGA3', 'WFTM':'HSFTM', 'WFTA':'HSFTA', \n",
    "       'WOR':'HSOR', 'WDR':'HSDR', 'WAst':'HSAst', \n",
    "       'WTO':'HSTO', 'WStl':'HSStl', 'WBlk':'HSBlk', \n",
    "       'WPF':'HSPF', \"WScore\":\"HS_Score\", \n",
    "       \n",
    "       'LFGM':'LSFGM', 'LFGA':'LSFGA', 'LFGM3':'LSFGM3', \n",
    "       'LFGA3':'LSFGA3', 'LFTM':'LSFTM', 'LFTA':'LSFTA', \n",
    "       'LOR':'LSOR', 'LDR':'LSDR', 'LAst':'LSAst', \n",
    "       'LStl':'LSStl', 'LBlk':'LSBlk', 'LPF':'LSPF', \n",
    "        'LTO':'LSTO', \"LScore\":\"LS_Score\", \n",
    "       \n",
    "       \n",
    "       \n",
    "       'lteam_seed':'ls_seed', 'wteam_seed':'ws_seed'}\n",
    "    \n",
    "    up = {'Season':'Season', 'DayNum':'DayNum', 'WTeamID':'LSTeamID', \n",
    "       'LTeamID':'HSTeamID', 'WLoc':'WLoc', 'NumOT':'NumOT', \n",
    "      \n",
    "       'WFGM':'LSFGM', 'WFGA':'LSFGA', 'WFGM3':'LSFGM3', \n",
    "       'WFGA3':'LSFGA3', 'WFTM':'LSFTM', 'WFTA':'LSFTA', \n",
    "       'WOR':'LSOR', 'WDR':'LSDR', 'WAst':'LSAst', \n",
    "       'WTO':'LSTO', 'WStl':'LSStl', 'WBlk':'LSBlk', \n",
    "       'WPF':'LSPF', \"WScore\":\"LS_Score\", \n",
    "       \n",
    "       'LFGM':'HSFGM', 'LFGA':'HSFGA', 'LFGM3':'HSFGM3', \n",
    "       'LFGA3':'HSFGA3', 'LFTM':'HSFTM', 'LFTA':'HSFTA', \n",
    "       'LOR':'HSOR', 'LDR':'HSDR', 'LAst':'HSAst', \n",
    "       'LStl':'HSStl', 'LBlk':'HSBlk', 'LPF':'HSPF',\n",
    "       'LTO':'HSTO', \"LScore\":\"HS_Score\", \n",
    "       \n",
    "       'lteam_seed':'ls_seed', 'wteam_seed':'ws_seed'}\n",
    "    \n",
    "    cols = list(fav.values())\n",
    "    empty = pd.DataFrame(columns = cols)\n",
    "    \n",
    "    for i in range(len(df)):\n",
    "  \n",
    "        if df['wteam_seed'][i] < df['lteam_seed'][i]:\n",
    "            #then the favorite won, apply favorite dictionary \n",
    "            \n",
    "            row = df.iloc[[i]]\n",
    "            row = row.rename(columns = fav)\n",
    "            empty = pd.concat([empty, row])\n",
    "        else:\n",
    "            #apply upset_dictioanry \n",
    "            row = df.iloc[[i]]\n",
    "            row = row.rename(columns = up)\n",
    "            empty = pd.concat([empty, row])\n",
    "            \n",
    "    \n",
    "    empty = empty.drop(columns = ['index'])\n",
    "    return empty \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca501fe2-186d-4f19-ae0c-78af823d2366",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c3877087-c26e-4623-94ff-d402cef1978a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "def power_seed_2(df, psd):\n",
    "    '''\n",
    "    up until this point we have been using winning vs losing team id's \n",
    "    we now are transitioning into lower vs higher seed id. \n",
    "    '''\n",
    "    \n",
    "    high = []\n",
    "    low = []\n",
    "    for i in tqdm(range(len(df))):\n",
    "        low.append(psd[df['Season'][i]][df['LSTeamID'][i]])\n",
    "        high.append(psd[df['Season'][i]][df['HSTeamID'][i]])\n",
    "            #pass\n",
    "    df['LS_power_seed'] = low\n",
    "    df['HS_power_seed'] = high\n",
    "    \n",
    "    return df \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43778c4-3fe3-4f6b-afb6-f7364bb62454",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bb2b2116-7255-48a6-9a44-b7ed3f6c6551",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_historical_tourney_perc(df):\n",
    "    ''' so im at this problem where I cant really use team ID as a dummy column \n",
    "    if I want to do a supervized learning technique, the problem being that \n",
    "    I would have a massive amount of columns, which would then make inverting \n",
    "    it very difficult, I would get a massive number of empty columns, so my data\n",
    "    would get very sparce very quickly. \n",
    "    \n",
    "    the reason I wanted to use team id would be for the program to have some sort of \n",
    "    way to recognize teams that do well across the tournament, Duke, Kentucky, and \n",
    "    teams of that nature come to mind. \n",
    "    \n",
    "    Going to build a dictionary with historical win percentages in the tournament. \n",
    "    \n",
    "    we will start from 1985, and go until 2003, and that will be the win percentage for \n",
    "    teams going into the 2003 tournament. Then we will have each year from 2003 onwards \n",
    "    will be its own year, and will be updated according to the wins and losses of the previous \n",
    "    tournament.\n",
    "    \n",
    "    this way we do not bias the results by including games that have already happened \n",
    "    for historical tournaments. \n",
    "    ''' \n",
    "    \n",
    "    # first read in the compact results for the NCAA tournament going back to 1985 \n",
    "    \n",
    "    keys = list(range(2003, year_+1))\n",
    "    #print(keys)\n",
    "    t_perc = {}\n",
    "    for i in range(len(keys)):\n",
    "        #initialize dict \n",
    "        season = {}\n",
    "        #find historical seasons \n",
    "        c_tour = df[df['Season'] < keys[i]]\n",
    "        \n",
    "        year = keys[i]\n",
    "        \n",
    "        wins = c_tour['WTeamID'].value_counts().reset_index()\n",
    "        losses = c_tour['LTeamID'].value_counts().reset_index()\n",
    "        \n",
    "        w_l = pd.merge(left = wins, right = losses, on = 'index')\n",
    "        w_l['total_games'] = w_l['WTeamID']+w_l['LTeamID']\n",
    "        \n",
    "        w_l['win_perc'] = w_l['WTeamID']/w_l['total_games']\n",
    "        \n",
    "        w_l['Season'] = year\n",
    "        year_dict = dict(zip(w_l['index'], w_l['win_perc']))\n",
    "        \n",
    "        t_perc.update({year:year_dict})\n",
    "        \n",
    "        \n",
    "    return t_perc      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc4b719-2ea4-42eb-b080-dce216fcf7b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8852794e-3d7d-41bb-a601-d395bc534f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create page rank data \n",
    "def make_page_data(df):\n",
    "    \"\"\"\n",
    "    Parameters: takes in regular season compact details, \n",
    "    \n",
    "    we want to calculate page rank: \n",
    "    \n",
    "    Page rank is a google algorithm which is a network theory \n",
    "    result basically giving you the degree centrality for the \n",
    "    different nodes in the network. \n",
    "    \n",
    "    here our nodes are teams, and the edges are if they played. \n",
    "    we want to know which teams beat which teams who beat which \n",
    "    teams, there by we can get some sort of degree centrality for \n",
    "    the team that beat the most powerful teams... \n",
    "    \n",
    "    we need to get the yearly winners and loosers for the page rank\n",
    "    structure we have from a homework assignment i did back in like \n",
    "    2018. So we get the by year regular season data, store that in \n",
    "    individual sheets which will then be fed into our page rank \n",
    "    algorithm to give us a by year by team page rank value\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    for i in tqdm(range(2003, year_+1)):\n",
    "        season = df[df['Season'] == i]\n",
    "        \n",
    "        data = season[['WTeamID', \"LTeamID\"]]\n",
    "        data = data.rename(columns = {\"WTeamID\":'Winner', 'LTeamID':'Loser'})\n",
    "        data.to_csv('../data/page_rank_data/'+str(i)+\"_page_data.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa0bb29-a4ab-4f5f-86d6-dc17dac9e860",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "20c203dc-8f5b-4592-b8e4-d7c324d7e3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problems 1-2\n",
    "# Shout out to Dr Evans for making me do this homework lol \n",
    "\n",
    "class DiGraph:\n",
    "    \"\"\"A class for representing directed graphs via their adjacency matrices.\n",
    "\n",
    "    Attributes:\n",
    "        (fill this out after completing DiGraph.__init__().)\n",
    "    \"\"\"\n",
    "    # Problem 1\n",
    "    def __init__(self, A, labels=None):\n",
    "        \"\"\"Modify A so that there are no sinks in the corresponding graph,\n",
    "        then calculate Ahat. Save Ahat and the labels as attributes.\n",
    "\n",
    "        Parameters:\n",
    "            A ((n,n) ndarray): the adjacency matrix of a directed graph.\n",
    "                A[i,j] is the weight of the edge from node j to node i.\n",
    "            labels (list(str)): labels for the n nodes in the graph.\n",
    "                If None, defaults to [0, 1, ..., n-1].\n",
    "        \"\"\"\n",
    "        for i in range(len(A)):\n",
    "            if(np.allclose(A[:,i],np.zeros(len(A[0])))):\n",
    "                #remove sink\n",
    "                A[:,i] = 1\n",
    "        #create ahat\n",
    "        self.Ahat =  A / A.sum(axis=0)\n",
    "        #get length of A\n",
    "        self.n = len(A)\n",
    "        #if no labels\n",
    "        if labels == None:\n",
    "            #Set standard\n",
    "            self.labels = np.arange(0,len(A))\n",
    "        else:\n",
    "            #otherwise use labels\n",
    "            self.labels = labels\n",
    "            #throw an error if it dosn't match\n",
    "        if len(A) != len(self.labels):\n",
    "            raise ValueError(\"Number of labels is not equal to the number of nodes\")\n",
    "\n",
    "\n",
    "    # Problem 2\n",
    "    def linsolve(self, epsilon=0.85):\n",
    "        \"\"\"Compute the PageRank vector using the linear system method.\n",
    "\n",
    "        Parameters:\n",
    "            epsilon (float): the damping factor, between 0 and 1.\n",
    "\n",
    "        Returns:\n",
    "            dict(str -> float): A dictionary mapping labels to PageRank values.\n",
    "        \"\"\"\n",
    "        #find values\n",
    "        p = la.solve(np.identity(self.n)-epsilon*self.Ahat,(1-epsilon)*np.ones(self.n)/self.n)\n",
    "        #make a dictionary\n",
    "        dict = {self.labels[i]:p[i] for i in range(self.n)}\n",
    "        return dict\n",
    "\n",
    "    # Problem 2\n",
    "    def eigensolve(self, epsilon=0.85):\n",
    "        \"\"\"Compute the PageRank vector using the eigenvalue method.\n",
    "        Normalize the resulting eigenvector so its entries sum to 1.\n",
    "\n",
    "        Parameters:\n",
    "            epsilon (float): the damping factor, between 0 and 1.\n",
    "\n",
    "        Return:\n",
    "            dict(str -> float): A dictionary mapping labels to PageRank values.\n",
    "        \"\"\"\n",
    "        #make B\n",
    "        B = epsilon*self.Ahat+(1-epsilon)/self.n*np.ones((self.n,self.n))\n",
    "        #get eig stuff\n",
    "        eigvals, eigvects = la.eig(B)\n",
    "        #get the eig vector for the largest value\n",
    "        p = eigvects[:,0].real\n",
    "        #normalize it\n",
    "        p = p/p.sum()\n",
    "        #make a dictionary\n",
    "        D = {self.labels[i]:p[i] for i in range(self.n)}\n",
    "        #return the dictionary\n",
    "        return D\n",
    "\n",
    "    # Problem 2\n",
    "    def itersolve(self, epsilon=0.85, maxiter=100, tol=1e-12):\n",
    "        \"\"\"Compute the PageRank vector using the iterative method.\n",
    "\n",
    "        Parameters:\n",
    "            epsilon (float): the damping factor, between 0 and 1.\n",
    "            maxiter (int): the maximum number of iterations to compute.\n",
    "            tol (float): the convergence tolerance.\n",
    "\n",
    "        Return:\n",
    "            dict(str -> float): A dictionary mapping labels to PageRank values.\n",
    "        \"\"\"\n",
    "        t = 0\n",
    "        #set p\n",
    "        p = [1/self.n for n in range(self.n)]\n",
    "        #while loop\n",
    "        while t < maxiter:\n",
    "            t += 1\n",
    "            #get the next p\n",
    "            pt = epsilon*self.Ahat@p + (1 - epsilon)*np.ones(self.n)/self.n\n",
    "            #stop if it meets the stopping condition\n",
    "            if(la.norm(pt - p, ord=1) < tol):\n",
    "                break\n",
    "            #otherwise reset\n",
    "            p = pt\n",
    "        #create the dictionary\n",
    "        D = {self.labels[i]:pt[i] for i in range(self.n)}\n",
    "        #return the dicitonary\n",
    "        return D\n",
    "\n",
    "# Problem 3\n",
    "def get_ranks(d):\n",
    "    \"\"\"Construct a sorted list of labels based on the PageRank vector.\n",
    "\n",
    "    Parameters:\n",
    "        d (dict(str -> float)): a dictionary mapping labels to PageRank values.\n",
    "\n",
    "    Returns:\n",
    "        (list) the keys of d, sorted by PageRank value from greatest to least.\n",
    "    \"\"\"\n",
    "    #get the keys\n",
    "    keys = np.array(list(d.keys()))\n",
    "    #get the values\n",
    "    values = list(d.values())\n",
    "    #create a mask\n",
    "    mask = np.array(np.argsort(values)[::-1])\n",
    "    #return the stuff\n",
    "    return list(keys[mask])\n",
    "\n",
    "# Problem 5\n",
    "def rank_ncaa_teams(filename, epsilon=0.85):\n",
    "    \"\"\"Read the specified file and construct a graph where node j points to\n",
    "    node i with weight w if team j was defeated by team i in w games. Use the\n",
    "    DiGraph class and its itersolve() method to compute the PageRank values of\n",
    "    the teams, then rank them with get_ranks().\n",
    "\n",
    "    Each line of the file has the format\n",
    "        A,B\n",
    "    meaning team A defeated team B.\n",
    "\n",
    "    Parameters:\n",
    "        filename (str): the name of the data file to read.\n",
    "        epsilon (float): the damping factor, between 0 and 1.\n",
    "\n",
    "    Returns:\n",
    "        (list(str)): The ranked list of team names.\n",
    "    \"\"\"\n",
    "    labels = []\n",
    "    with open(filename,'r') as infile:\n",
    "        content = infile.read().strip()\n",
    "    #print(content)\n",
    "    labels = sorted(set(content.replace('\\n',',').split(',')))\n",
    "    labels.remove(\"Loser\")\n",
    "    labels.remove(\"Winner\")\n",
    "    #init dictionary\n",
    "    Dict = {team: i for i, team in enumerate(labels)}\n",
    "    #init adjacency matrix\n",
    "    A = np.zeros((len(labels), len(labels)))\n",
    "    #get the information from the lines\n",
    "    for line in content.split('\\n')[1:]:\n",
    "        teams = line.split(',')\n",
    "        row = Dict[teams[0]]\n",
    "        column = Dict[teams[1]]\n",
    "        A[row][column] += 1\n",
    "    D = DiGraph(A, labels=labels)\n",
    "    #get the page rank\n",
    "    PageRank_ = D.itersolve(epsilon = epsilon)\n",
    "    #return the stuff\n",
    "    #print(PageRank_)\n",
    "    return PageRank_, get_ranks(PageRank_)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dd7c5e90-91f0-4b83-997c-db65977344e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_page_rank():\n",
    "    '''\n",
    "    we take the page rank algorithm from above \n",
    "    which takes in the page rank data that we have saved \n",
    "    previously. \n",
    "    \n",
    "    it will then return a dictionary of seasons \n",
    "    which will have a dicitonary of teams and their \n",
    "    degree centrality (page rank), which we will then \n",
    "    use as a feature for our algorithm later on \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    page_data = {}\n",
    "    teams_rank = {}\n",
    "    path = '../data/page_rank_data/'\n",
    "    for i in range(2003, year_+1):\n",
    "        full_path = path+str(i)+\"_page_data.csv\"\n",
    "        #year_data = pd.read_csv(full_path)\n",
    "        \n",
    "        year_page, ranked = rank_ncaa_teams(full_path)\n",
    "        \n",
    "        page_data.update({i:year_page})\n",
    "        teams_rank.update({i:ranked})\n",
    "        \n",
    "    return page_data, teams_rank\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e3fd74-0f69-4d85-98e6-5fed604644a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1dd35f-704e-4f70-bfbd-8b6be625cb9d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "33469758-4085-4899-badc-92665015f6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_round(df):\n",
    "    '''\n",
    "    takes in tournament data, assigns a categorical \n",
    "    variable based on DayNum for the round \n",
    "    \n",
    "    DayNum=136 or 137 (Thu/Fri) - Round 1, to bring the tournament field from 64 teams to 32 teams\n",
    "    DayNum=138 or 139 (Sat/Sun) - Round 2, to bring the tournament field from 32 teams to 16 teams\n",
    "    DayNum=143 or 144 (Thu/Fri) - Round 3, otherwise known as \"Sweet Sixteen\", to bring the tournament field from 16 teams to 8 teams\n",
    "    DayNum=145 or 146 (Sat/Sun) - Round 4, otherwise known as \"Elite Eight\" or \"regional finals\", to bring the tournament field from 8 teams to 4 teams\n",
    "    DayNum=152 (Sat) - Round 5, otherwise known as \"Final Four\" or \"national semifinals\", to bring the tournament field from 4 teams to 2 teams\n",
    "    DayNum=154 (Mon) - Round 6, otherwise known as \"national final\" or \"national championship\", to bring the tournament field from 2 teams to 1 champion team\n",
    "    '''\n",
    "    \n",
    "    df['first_round'] = df['DayNum'].apply(lambda x: 1 if x in [136, 137] else 0)\n",
    "    df['second_round'] = df['DayNum'].apply(lambda x: 1 if x in [138, 139] else 0)\n",
    "    df['sweet_16'] = df['DayNum'].apply(lambda x: 1 if x in [143, 144] else 0)\n",
    "    df['elite_8'] = df['DayNum'].apply(lambda x: 1 if x in [145, 146] else 0)\n",
    "    df['final_four'] = df['DayNum'].apply(lambda x: 1 if x == 152 else 0)\n",
    "    df['championship'] = df['DayNum'].apply(lambda x: 1 if x == 154 else 0)\n",
    "    \n",
    "    return df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49289aca-0c47-4982-a245-985f54ec27b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c70ea712-c99c-4f30-9770-4a34602a92c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reg_avgs2(df, total_dict):\n",
    "    \"\"\"\n",
    "    Parameters: Tournament details that has gone through the \n",
    "    cleaning and prepping phases highlighted above \n",
    "    \n",
    "    ADDITIONALLY: Although not a direct input, but another parameter is the total_dict \n",
    "    that we have build to have the regular season averages by team\n",
    "    by season, that will be used to fill the different features, \n",
    "    change those from the actual \"in game\" values to regular season \n",
    "    average values, and then we will add some more features here and \n",
    "    a few later on. \n",
    "    \n",
    "    Creating the features that we want for our modeling df --> \n",
    "    ready go! \n",
    "    \n",
    "    \"\"\"  \n",
    "    df['HS_avg_score'] = df.apply(lambda x: total_dict[x['HSTeamID']][x['Season']]['Score'], axis = 1)\n",
    "    df['HS_avg_against'] = df.apply(lambda x: total_dict[x['HSTeamID']][x['Season']]['Op_score'], axis = 1)\n",
    "    \n",
    "    df['LS_avg_score'] = df.apply(lambda x: total_dict[x['LSTeamID']][x['Season']]['Score'], axis = 1)\n",
    "    df['LS_avg_against'] = df.apply(lambda x: total_dict[x['LSTeamID']][x['Season']]['Op_score'], axis = 1)\n",
    "    \n",
    "    \n",
    "    df['HS_wins'] =  df.apply(lambda x: total_dict[x['HSTeamID']][x['Season']]['num_wins'], axis = 1)\n",
    "    df['HS_loss'] =  df.apply(lambda x: total_dict[x['HSTeamID']][x['Season']]['num_loss'], axis = 1)\n",
    "    \n",
    "    df['LS_wins'] = df.apply(lambda x: total_dict[x['LSTeamID']][x['Season']]['num_wins'], axis = 1)\n",
    "    df['LS_loss'] = df.apply(lambda x: total_dict[x['LSTeamID']][x['Season']]['num_loss'], axis = 1)\n",
    "    \n",
    "    df['HS_op_FGM'] = df.apply(lambda x: total_dict[x['HSTeamID']][x['Season']]['Op_FGM'], axis = 1)\n",
    "    df['HS_op_FGA'] =df.apply(lambda x: total_dict[x['HSTeamID']][x['Season']]['Op_FGA'], axis = 1)\n",
    "    df['HS_op_FMG3'] = df.apply(lambda x: total_dict[x['HSTeamID']][x['Season']]['Op_FGM3'], axis = 1)\n",
    "    df['HS_op_FGA3'] = df.apply(lambda x: total_dict[x['HSTeamID']][x['Season']]['Op_FGA3'], axis = 1)\n",
    "    df['HS_op_OR'] = df.apply(lambda x: total_dict[x['HSTeamID']][x['Season']]['Op_OR'], axis = 1)\n",
    "    df['HS_op_DR'] =  df.apply(lambda x: total_dict[x['HSTeamID']][x['Season']]['Op_DR'], axis = 1)\n",
    "    df['HS_op_To'] = df.apply(lambda x: total_dict[x['HSTeamID']][x['Season']]['Op_TO'], axis = 1)\n",
    "    \n",
    "    df['LS_op_FGM'] = df.apply(lambda x: total_dict[x['LSTeamID']][x['Season']]['Op_FGM'], axis = 1)\n",
    "    df['LS_op_FGA'] =df.apply(lambda x: total_dict[x['LSTeamID']][x['Season']]['Op_FGA'], axis = 1)\n",
    "    df['LS_op_FMG3'] = df.apply(lambda x: total_dict[x['LSTeamID']][x['Season']]['Op_FGM3'], axis = 1)\n",
    "    df['LS_op_FGA3'] = df.apply(lambda x: total_dict[x['LSTeamID']][x['Season']]['Op_FGA3'], axis = 1)\n",
    "    df['LS_op_OR'] = df.apply(lambda x: total_dict[x['LSTeamID']][x['Season']]['Op_OR'], axis = 1)\n",
    "    df['LS_op_DR'] =  df.apply(lambda x: total_dict[x['LSTeamID']][x['Season']]['Op_DR'], axis = 1)\n",
    "    df['LS_op_To'] = df.apply(lambda x: total_dict[x['LSTeamID']][x['Season']]['Op_TO'], axis = 1)\n",
    "    \n",
    "    \n",
    "    df['HSFGM'] = df.apply(lambda x: total_dict[x['HSTeamID']][x['Season']]['FGM'], axis = 1)\n",
    "    df['HSFGA'] = df.apply(lambda x: total_dict[x['HSTeamID']][x['Season']]['FGA'], axis = 1)\n",
    "    df['HSFGM3'] = df.apply(lambda x: total_dict[x['HSTeamID']][x['Season']]['FGM3'], axis = 1)\n",
    "    df['HSFGA3'] = df.apply(lambda x: total_dict[x['HSTeamID']][x['Season']]['FGA3'], axis = 1)\n",
    "    df['HSFTM'] = df.apply(lambda x: total_dict[x['HSTeamID']][x['Season']]['FTM'], axis = 1)\n",
    "    df['HSFTA'] = df.apply(lambda x: total_dict[x['HSTeamID']][x['Season']]['FTA'], axis = 1)\n",
    "    df['HSOR'] = df.apply(lambda x: total_dict[x['HSTeamID']][x['Season']]['OR'], axis = 1)\n",
    "    df['HSDR'] = df.apply(lambda x: total_dict[x['HSTeamID']][x['Season']]['DR'], axis = 1)\n",
    "    df['HSAst'] = df.apply(lambda x: total_dict[x['HSTeamID']][x['Season']]['Ast'], axis = 1)\n",
    "    df['HSStl'] = df.apply(lambda x: total_dict[x['HSTeamID']][x['Season']]['Stl'], axis = 1)\n",
    "    df['HSBlk'] = df.apply(lambda x: total_dict[x['HSTeamID']][x['Season']]['Blk'], axis = 1)\n",
    "    df['HSPF'] = df.apply(lambda x: total_dict[x['HSTeamID']][x['Season']]['PF'], axis = 1)\n",
    "    \n",
    "    \n",
    "    df['LSFGM'] = df.apply(lambda x: total_dict[x['LSTeamID']][x['Season']]['FGM'], axis = 1)\n",
    "    df['LSFGA'] = df.apply(lambda x: total_dict[x['LSTeamID']][x['Season']]['FGA'], axis = 1)\n",
    "    df['LSFGM3'] = df.apply(lambda x: total_dict[x['LSTeamID']][x['Season']]['FGM3'], axis = 1)\n",
    "    df['LSFGA3'] = df.apply(lambda x: total_dict[x['LSTeamID']][x['Season']]['FGA3'], axis = 1)\n",
    "    df['LSFTM'] = df.apply(lambda x: total_dict[x['LSTeamID']][x['Season']]['FTM'], axis = 1)\n",
    "    df['LSFTA'] = df.apply(lambda x: total_dict[x['LSTeamID']][x['Season']]['FTA'], axis = 1)\n",
    "    df['LSFTA'] = df.apply(lambda x: total_dict[x['LSTeamID']][x['Season']]['OR'], axis = 1)\n",
    "    df['LSDR'] = df.apply(lambda x: total_dict[x['LSTeamID']][x['Season']]['DR'], axis = 1)\n",
    "    df['LSAst'] = df.apply(lambda x: total_dict[x['LSTeamID']][x['Season']]['Ast'], axis = 1)\n",
    "    df['LSStl'] = df.apply(lambda x: total_dict[x['LSTeamID']][x['Season']]['Stl'], axis = 1)\n",
    "    df['LSBlk'] = df.apply(lambda x: total_dict[x['LSTeamID']][x['Season']]['Blk'], axis = 1)\n",
    "    df['LSPF'] = df.apply(lambda x: total_dict[x['LSTeamID']][x['Season']]['PF'], axis = 1)\n",
    "    \n",
    "\n",
    "    #drop columns we dont need \n",
    "    df = df.drop(columns = ['DayNum', 'WLoc', 'NumOT', 'ls_seed', 'ws_seed'])   \n",
    "    return df      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a4f8f92-f9fc-42a5-8e81-eb409cf73e1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5cbcab35-9a45-421d-b29e-dd692c677764",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_conf_data(df):\n",
    "    '''\n",
    "    Parameters: takes in data frame from previous function with features almost \n",
    "    ready for trainign, \n",
    "    \n",
    "    uses conf_champ dictionary and conf_affil dictionaries to get conference afiliation \n",
    "    and if the teams in question were conference champions. \n",
    "    \n",
    "    NOTE: the ivy league only started playing a conference tournament in 2017\n",
    "    so if not found insert ivy lol \n",
    "    '''\n",
    "    #https://torchcollegerecruiting.com/z_a3_high_mid_low_majors/1641907452726x394612841469246460\n",
    "    # high vs medium vs low major conferences \n",
    "\n",
    "    power_conf = [\"acc\", 'sec', \"big_ten\", \n",
    "                  \"big_twelve\", \"pac_ten\", \"pac_twelve\"]\n",
    "\n",
    "    mid_major = ['cusa', 'aac', 'mwc', 'sun_belt', 'ivy', \n",
    "                 'mac', 'big_sky', 'meac' ,'southland', \n",
    "                 'summit', 'wac', 'wcc',]\n",
    "\n",
    "    low_major = ['aec', 'a_ten', 'big_south', 'caa', \n",
    "                 'nec', 'patriot', 'southern', 'swac', \n",
    "                 'mvc', 'a_sun', 'ovc', 'horizon', \n",
    "                 'maac', 'swac']    \n",
    "    \n",
    "    \n",
    "    hs_conf= []\n",
    "    ls_conf = []\n",
    "    \n",
    "    hs_conf_champ = []\n",
    "    ls_conf_champ = []\n",
    "    for i in tqdm(range(len(df))):\n",
    "        try:\n",
    "            hs_conf.append(conf_affil[df['Season'][i]][df['HSTeamID'][i]])\n",
    "\n",
    "        except:\n",
    "            hs_conf.append('ivy')\n",
    "\n",
    "        try:\n",
    "            ls_conf.append(conf_affil[df['Season'][i]][df['LSTeamID'][i]]) \n",
    "        except:\n",
    "            ls_conf.append('ivy')\n",
    "        \n",
    "        \n",
    "        if df['HSTeamID'][i] in list(conf_champs[df['Season'][i]].keys()):\n",
    "            hs_conf_champ.append(1)\n",
    "        else:\n",
    "            hs_conf_champ.append(0)\n",
    "        \n",
    "        if df['LSTeamID'][i] in list(conf_champs[df['Season'][i]].keys()):\n",
    "            ls_conf_champ.append(1)\n",
    "        else:\n",
    "            ls_conf_champ.append(0)\n",
    "        \n",
    "        \n",
    "    \n",
    "    df['HS_Conf'] = hs_conf\n",
    "    df['LS_Conf'] = ls_conf\n",
    "    \n",
    "    df['HS_power_conf']= df['HS_Conf'].apply(lambda x: 1 if x in power_conf else 0)\n",
    "    df['HS_mid_conf']= df['HS_Conf'].apply(lambda x: 1 if x in mid_major else 0)\n",
    "    df['HS_low_conf'] = df['HS_Conf'].apply(lambda x: 1 if x in low_major else 0)\n",
    "    \n",
    "    df['LS_power_conf']= df['LS_Conf'].apply(lambda x: 1 if x in power_conf else 0)\n",
    "    df['LS_mid_conf']= df['LS_Conf'].apply(lambda x: 1 if x in mid_major else 0)\n",
    "    df['LS_low_conf'] = df['LS_Conf'].apply(lambda x: 1 if x in low_major else 0)    \n",
    "\n",
    "    df['HS_conf_champ'] = hs_conf_champ\n",
    "    df['LS_conf_champ'] = ls_conf_champ\n",
    "    \n",
    "    \n",
    "    df = df.drop(columns = ['HS_Conf', 'LS_Conf',])\n",
    "    return df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52972f54-1b81-441a-82b5-d97d12d935d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dad91a45-412c-450e-8e36-bb553bfbae42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_page_rank(df):\n",
    "    '''\n",
    "    Parameters: tournament DF in the preperation phase for modeling, \n",
    "    ADDITIONALLY: page_rank dictionary\n",
    "    \n",
    "    Having completed the page rank steps above, we have a page rank dictionary\n",
    "    \"page_rank\", which has the by season by team values for the different teams, \n",
    "    we will use this to populate page rank for low and high seeds \n",
    "    \n",
    "    '''\n",
    "    #df['HS_page_rank'] = np.zeros(len(df))\n",
    "    #df['LS_page_rank'] = np.zeros(len(df))\n",
    "    \n",
    "        #print(i)\n",
    "    df['HS_page_rank'] = df.apply(lambda x: page_rank[x['Season']][str(x['HSTeamID'])], axis = 1)\n",
    "    df['LS_page_rank'] = df.apply(lambda x: page_rank[x['Season']][str(x['LSTeamID'])], axis = 1)\n",
    "    return df     \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3fb05a5-cc48-42e3-aba4-1ead624ccc4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2990843d-9662-4e9c-8045-4fc979cb224e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hist_wins(df):\n",
    "    '''\n",
    "    Parameters: tournament DF in the preperation phase for modeling, \n",
    "    ADDITIONALLY: tw (tournament wins) dictionary\n",
    "    \n",
    "    having discussed this above, we probably cannot use teamID's as \n",
    "    features simply because those will be constantly changing throughout the rounds \n",
    "    and years of the tournament, makign it difficult to have consistent features if \n",
    "    we trim down / out teams that have been elemenated, but also the dimmensionality \n",
    "    will be enormous and be very sparce, so instead we will use historical tournament \n",
    "    winning percentage to try and capture some of that \"recognize me I've won in the \n",
    "    tournament a lot\" vibe we might be able to get from team_id \n",
    "    \n",
    "    '''\n",
    "    df['HS_historical_tournament_win%'] = np.zeros(len(df))\n",
    "    df['LS_historical_tournament_win%'] = np.zeros(len(df))\n",
    "    \n",
    "    \n",
    "    for i in tqdm(range(len(df))):\n",
    "        try:\n",
    "            df['HS_historical_tournament_win%'][i] = tw[df['Season'][i]][df['HSTeamID'][i]]\n",
    "            df['LS_historical_tournament_win%'][i] = tw[df['Season'][i]][df['LSTeamID'][i]]\n",
    "        except:\n",
    "            df['LS_historical_tournament_win%'][i] = 0\n",
    "        \n",
    "    return df \n",
    "                                                                \n",
    "                                                    \n",
    "                                                                  \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc5c823-0292-426d-8eaa-0c3a32873749",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a80be039-23b7-4636-8381-450bbc2e3705",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_target(df):\n",
    "    '''\n",
    "    Parameters: DF in prep stage for modeling\n",
    "    \n",
    "    with the feature building steps finally complete, \n",
    "    our last feature is for us to create our target variabe \n",
    "    did the higher seed win or lose? \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    df['target'] = df.apply(lambda x: 1 if x['HS_Score'] > x['LS_Score'] else 0, axis = 1)\n",
    "    \n",
    "    return df \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381cb0c1-bc83-4023-9796-ea93d1226361",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911c4c13-4de3-458d-a6c6-fda371c6aa3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02808d5-4284-43d4-9105-15143dd7b015",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0012a4f-3879-4377-978b-ffbedeb3575b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5cdde3eb-074f-4aa3-9c09-38b05023f618",
   "metadata": {},
   "outputs": [],
   "source": [
    "# corr_mat = tour9.corr()\n",
    "# f, ax = plt.subplots(figsize = (10, 6))\n",
    "# sns.heatmap(corr_mat,vmax=.8,square=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff610b07-a855-4c5d-a22a-e4a690dc3ed4",
   "metadata": {},
   "source": [
    "# Training and validation phase "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6b6dd201-96e4-4783-9d70-2bc518d1ae25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train seasons: [2013, 2007, 2015, 2004, 2005, 2006, 2008, 2012, 2003, 2011, 2018, 2021, 2020, 2009, 2023, 2016]\n",
      "val seasons: [2017, 2019, 2022, 2010, 2014]\n"
     ]
    }
   ],
   "source": [
    "seasons = list(range(2003, year_))\n",
    "\n",
    "random.seed(7)\n",
    "train_seasons = random.sample(seasons, 16)\n",
    "print(\"train seasons:\", train_seasons)\n",
    "val_seasons = list(set(seasons)-set(train_seasons))\n",
    "print(\"val seasons:\", val_seasons)\n",
    "#get rid of 2020 in val season - no tournament that year "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1ab285cc-9518-4a0e-a98b-232c050570d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_train_val(df):\n",
    "    \n",
    "    df['train'] = df['Season'].apply(lambda x: 1 if x in train_seasons else 0)\n",
    "    df['validate'] = df['Season'].apply(lambda x: 1 if x in val_seasons else 0)\n",
    "    \n",
    "    train = df[df['train'] == 1]\n",
    "    val = df[df['validate']==1]\n",
    "    \n",
    "    val = val.drop(columns = ['validate', 'train', \"Season\",\n",
    "                              #\"HSTeamID\",\"LSTeamID\",\n",
    "                              'LS_Score', \"HS_Score\", ])\n",
    "    train = train.drop(columns = ['train','validate', \"Season\",\n",
    "                                  #\"HSTeamID\",\"LSTeamID\",\n",
    "                                  'LS_Score', \"HS_Score\", ])\n",
    "    \n",
    "    \n",
    "    return val, train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0347f3a7-b051-4b07-9ba8-4889ea516a91",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 24/24 [00:00<00:00, 5891.22it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 369/369 [00:08<00:00, 42.74it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 369/369 [00:10<00:00, 35.14it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 379/379 [01:35<00:00,  3.97it/s]\n"
     ]
    }
   ],
   "source": [
    "reg_deets2 = get_conf_affiliation()\n",
    "conf_champs, cdf2 = get_conf_dict(conf_tour)\n",
    "page_rank, teams_rank = get_page_rank()\n",
    "w_feats = get_winning_dict(reg_deets, \"WTeamID\")\n",
    "l_feats = get_losing_dict(reg_deets, \"LTeamID\")\n",
    "total_dict = combine2(w_feats, l_feats)\n",
    "conf_affil = get_conf_affiliation_dict(reg_deets2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f4b6fb05-1c6a-4c67-979c-06a8327bcf89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_train(input_combinations):\n",
    "    \n",
    "    #conf_champs, cdf2 = get_conf_dict(conf_tour)\n",
    "    reg_deets2 = get_conf_affiliation()\n",
    "    \n",
    "    for i in range(len(input_combinations)):\n",
    "        print(i/len(input_combinations))\n",
    "        \n",
    "        #reg_deets2 = get_conf_affiliation()\n",
    "    \n",
    "#         w_feats = get_winning_dict(reg_deets, \"WTeamID\")\n",
    "#         l_feats = get_losing_dict(reg_deets, \"LTeamID\")\n",
    "#         total_dict = combine2(w_feats, l_feats)\n",
    "        \n",
    "        \n",
    "        \n",
    "#         conf_affil = get_conf_affiliation_dict(reg_deets2)\n",
    "        power_seeds = put_in_first_four(power_seeds_df, input_combinations[i])\n",
    "        psd  = create_power_seeds_dict(power_seeds)\n",
    "        \n",
    "        #p5 = put_in_first_four(power_seeds_df, combos[15])\n",
    "        #ps5 = create_power_seeds_dict(p5)\n",
    "\n",
    "        tour2 = get_power_seeds_tourney(tourney_deets, psd)\n",
    "        tour3 = prep_data(tour2)\n",
    "        tour3 = power_seed_2(tour3, psd)\n",
    "        tw = get_historical_tourney_perc(compact)\n",
    "        make_page_data(reg_deets)\n",
    "        \n",
    "\n",
    "        tour4 = get_round(tour3)\n",
    "        tour5 = get_reg_avgs2(tour4, total_dict)\n",
    "        tour6 = get_conf_data(tour5)\n",
    "        tour7 = apply_page_rank(tour6)\n",
    "        tour8 = get_hist_wins(tour7)\n",
    "        tour9 = get_target(tour8)\n",
    "        val, train = make_train_val(tour9)\n",
    "        \n",
    "        out_str = str(input_combinations[i]).replace(\"'\", \"\").replace(\"[\",\"\").replace(\", \", \"_\").replace(\"]\",\"\")\n",
    "        \n",
    "        train.to_csv('../train_data/'+out_str+'.csv')\n",
    "        val.to_csv('../val_data/'+out_str+'.csv')\n",
    "        \n",
    "    \n",
    "    print(\"finished generating training data\") \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e62bcbe5-b7ed-41e7-be92-5d7fdc0c34f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 955/955 [00:00<00:00, 48035.79it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 955/955 [00:00<00:00, 46646.80it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 60.58it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 955/955 [00:00<00:00, 22669.09it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 955/955 [00:00<00:00, 3522.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 955/955 [00:00<00:00, 48592.89it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 955/955 [00:00<00:00, 39358.95it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 58.56it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 955/955 [00:00<00:00, 21656.83it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 955/955 [00:00<00:00, 3209.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 955/955 [00:00<00:00, 37665.01it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 955/955 [00:00<00:00, 35164.56it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 58.89it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 955/955 [00:00<00:00, 12674.22it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 955/955 [00:00<00:00, 2912.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 955/955 [00:00<00:00, 48766.82it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 955/955 [00:00<00:00, 48036.36it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 62.63it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 955/955 [00:00<00:00, 23301.69it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 955/955 [00:00<00:00, 3275.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 955/955 [00:00<00:00, 42911.81it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 955/955 [00:00<00:00, 47202.54it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 68.93it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 955/955 [00:00<00:00, 22980.44it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 955/955 [00:00<00:00, 3444.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 955/955 [00:00<00:00, 48079.03it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 955/955 [00:00<00:00, 45158.01it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 60.21it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 955/955 [00:00<00:00, 21884.00it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 955/955 [00:00<00:00, 3208.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 955/955 [00:00<00:00, 40019.99it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 955/955 [00:00<00:00, 48128.14it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 68.36it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 955/955 [00:00<00:00, 23003.14it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 955/955 [00:00<00:00, 3381.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 955/955 [00:00<00:00, 50162.93it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 955/955 [00:00<00:00, 48060.00it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 65.19it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 955/955 [00:00<00:00, 22987.17it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 955/955 [00:00<00:00, 3465.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 955/955 [00:00<00:00, 48496.99it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 955/955 [00:00<00:00, 47953.55it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 64.92it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 955/955 [00:00<00:00, 23349.09it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 955/955 [00:00<00:00, 3503.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 955/955 [00:00<00:00, 48467.65it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 955/955 [00:00<00:00, 47436.76it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 69.50it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 955/955 [00:00<00:00, 22788.52it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 955/955 [00:00<00:00, 3539.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 955/955 [00:00<00:00, 48854.25it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 955/955 [00:00<00:00, 47571.41it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 64.06it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 955/955 [00:00<00:00, 22984.40it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 955/955 [00:00<00:00, 3549.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 955/955 [00:00<00:00, 49356.91it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 955/955 [00:00<00:00, 46614.77it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 59.46it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 955/955 [00:00<00:00, 22583.84it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 955/955 [00:00<00:00, 3520.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 955/955 [00:00<00:00, 48388.60it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 955/955 [00:00<00:00, 47776.24it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 66.41it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 955/955 [00:00<00:00, 22747.88it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 955/955 [00:00<00:00, 3562.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 955/955 [00:00<00:00, 49049.88it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 955/955 [00:00<00:00, 47441.82it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 69.48it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 955/955 [00:00<00:00, 22761.45it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 955/955 [00:00<00:00, 3594.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 955/955 [00:00<00:00, 49029.47it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 955/955 [00:00<00:00, 47019.14it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 64.25it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 955/955 [00:00<00:00, 23043.11it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 955/955 [00:00<00:00, 3568.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 955/955 [00:00<00:00, 49527.79it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 955/955 [00:00<00:00, 47503.15it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 69.13it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 955/955 [00:00<00:00, 22939.65it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 955/955 [00:00<00:00, 3587.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished generating training data\n"
     ]
    }
   ],
   "source": [
    "make_train(combos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377b6e3f-324c-45a2-8860-0f888ec4f502",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2709fdfc-36a1-48c8-84e9-ddb5aa714a18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5958615-6173-41af-a3aa-55255b2dacd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d59b4a-c981-4ba7-8673-0970047229cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd5a7b0-4047-4bc6-a663-d983936fe9fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
