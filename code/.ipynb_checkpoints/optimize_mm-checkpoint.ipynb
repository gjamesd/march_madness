{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9b7b288-4216-4e29-b88c-7e8b327acdef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing LOTS of stuff \n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import ElasticNetCV\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import f1_score, mean_squared_error, classification_report \n",
    "from sklearn.model_selection import KFold, RepeatedKFold, GroupKFold, GridSearchCV, train_test_split\n",
    "import random \n",
    "from random import sample\n",
    "\n",
    "#from online suggestions I was reading \n",
    "from string import ascii_letters\n",
    "import statsmodels.api as sm # for linear regression, found it on some article \n",
    "\n",
    "# to solve problems that I am encounterinbg\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt #plotting for visualization purposes of story telling \n",
    "import numpy as np\n",
    "\n",
    "#from tqdm import tqdm\n",
    "\n",
    "import datetime\n",
    "import gc\n",
    "import seaborn as sns\n",
    "sns.set_palette(sns.color_palette('hls', 7))\n",
    "\n",
    "from statistics import mean\n",
    "from collections import Counter\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "from collections import Counter\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "from scipy import linalg as la\n",
    "import networkx as nx\n",
    "from itertools import combinations\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea598c9-d411-4779-bffe-f11e76d4c11b",
   "metadata": {},
   "source": [
    "# Step 0 - read in data and libraries for process \n",
    "\n",
    "for a full data description from kaggle, visit this webstie: https://www.kaggle.com/competitions/mens-march-mania-2022/data\n",
    "\n",
    "The only thing i created by hand is Power the power seeding dictionary. The selection comitte does this, selects teams seeds the tournament. The top overal seed is obviously 1, the second 1 seed, on the opposite side of the bracket is 2, on the same side of the bracket is the third overall and then the fourth overall 1 seed is on the same side as the 1st overall seed. This means that the 5th overall seed is the \"best\" 2 seed and get the \"wrost\" 1 seed. and then the process is cyclic, rotating around the tournament from 1 to 64. This is the variable in question I use to generate the outcome variable: Did the higher seed win? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "ca8ab4ce-3ece-466f-98f1-89aaeecdf419",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import lots of data \n",
    "# teams that could make the tournament all D1 teams \n",
    "teams_df = pd.read_csv(\"../data/MTeams.csv\")\n",
    "\n",
    "#detailed game by game of tournament games from 2003 onward \n",
    "tourney_deets = pd.read_csv(\"../data/MNCAATourneyDetailedResults.csv\")\n",
    "\n",
    "#regular season games from 2003 onwards \n",
    "reg_deets = pd.read_csv(\"../data/MRegularSeasonDetailedResults.csv\")\n",
    "\n",
    "#winner and loser of tournament games since 1985 \n",
    "tourney_compact_deets =  pd.read_csv(\"../data/MNCAATourneyCompactResults.csv\")\n",
    "\n",
    "#historical tournament seeds \n",
    "tourney_seeds = pd.read_csv(\"../data/MNCAATourneySeeds.csv\")\n",
    "\n",
    "#conference tournament games detailed results \n",
    "conf_tour = pd.read_csv(\"../data/MConferenceTourneyGames.csv\")\n",
    "\n",
    "#what conferences had who in what year \n",
    "conf = pd.read_csv(\"../data/MTeamConferences.csv\")\n",
    "\n",
    "#hand made varialbe used to create target \n",
    "power_seeds = pd.read_csv(\"../data/PowerSeeds.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c78891-11ad-4bec-bb17-d7be46f3c9a7",
   "metadata": {},
   "source": [
    "# Step 1 Data Prep / build dictionaries for features \n",
    "\n",
    "The goal here is to use among other things regular season values for each team for each season as features for the model. We will create a multitude of different dictionaries with seasons and teams as keys and things like average 3's made, average turnovers, number of wins etc as values. This will be the general approach we take to model feature creation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3cdf1f14-f453-4706-92c1-06ad8e79f8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_conf_affiliation():\n",
    "    '''\n",
    "    Parameters: None\n",
    "    output: regular season detailed results with the winning \n",
    "    and losing team conference afiiliation denoted \n",
    "    \n",
    "    prepping a data frame for getting conference affiliation \n",
    "    by season by year here below\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    step1 = pd.merge(left = reg_deets, right = conf_tour, on = ['Season', \"WTeamID\"]) \n",
    "    \n",
    "    step1 = step1.drop(columns = ['DayNum_y', 'LTeamID_y'])\n",
    "    step1 = step1.rename(columns = {'DayNum_x':'DayNum',\n",
    "                                   'ConfAbbrev':\"WTeam_Conf\", \n",
    "                                   \"LTeamID_x\":\"LTeamID\"})\n",
    "    \n",
    "    step2 = pd.merge(left = step1, right = conf_tour, on = ['Season', \"LTeamID\"]) \n",
    "    step2 = step2.drop(columns = ['DayNum_y', 'WTeamID_y'])\n",
    "    step2 = step2.rename(columns = {'DayNum_x':'DayNum','ConfAbbrev':\"LTeam_Conf\",\n",
    "                                   'WTeamID_x':'WTeamID'})\n",
    "    \n",
    "    return step2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c98d742f-6b07-4595-b917-782cc4f2ad56",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_deets2 = get_conf_affiliation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c3ccf26b-0a80-4137-b09d-3db02138ee4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_winning_dict(df, column):\n",
    "    '''\n",
    "    building a dictionary for the averages by season by team for when they \n",
    "    were the winning team: \n",
    "    \n",
    "    Note - it is possible for a team to go winless, we will need to \n",
    "    figure out how to deal with that should we have an undefeated regular season--\n",
    "    although these teams will not play in march and it seams very unlikely that \n",
    "    they don't get a single win, it is possible lol \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    features = ['WScore', 'LScore', 'WFGM', 'WFGA', 'WFGM3','WFGA3', 'WFTM', 'WFTA', 'WOR',\n",
    "                'WTO',\n",
    "            'WDR', 'WAst', 'WStl', 'WBlk', 'WPF', 'LFGM', 'LFGA', 'LFGM3', 'LFGA3',\n",
    "       'LFTM', 'LFTA', 'LOR', 'LDR', 'LAst', 'LTO', 'LStl', 'LBlk', 'LPF', ]\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "    w = df.groupby(by=[column])\n",
    "    w1 = pd.DataFrame(w)\n",
    "    teams = {}\n",
    "    \n",
    "    for i in tqdm(range(len(w1))):\n",
    "        #print(i)\n",
    "        ws = w1[1][i].groupby(\"Season\")\n",
    "        ws = pd.DataFrame(ws)\n",
    "        #print(ws['Season'])\n",
    "        seasons = {}\n",
    "        for j in range(len(ws)):\n",
    "            season = ws[0][j]\n",
    "            #print(season)\n",
    "            #vals = dict(ws[1][j][features].sum(axis = 0)/len(ws[1][j][features]))\n",
    "            vals = dict(ws[1][j][features].sum(axis = 0))\n",
    "            vals.update({'season_wins': len(ws[1][j])})\n",
    "            #vals.update({'conf':list(set(ws[1][j]['WTeam_Conf']))[0]})\n",
    "            #print(v_dict)\n",
    "            seasons.update({ws[0][j]:  vals})\n",
    "        \n",
    "        teams.update({w1[0][i]:seasons})\n",
    "    return teams\n",
    "        \n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3376a586-8451-441f-87a2-d42c7c9532cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████| 363/363 [00:09<00:00, 38.23it/s]\n"
     ]
    }
   ],
   "source": [
    "w_feats = get_winning_dict(reg_deets, \"WTeamID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94f8e54-b2ba-4eb3-addb-3dc6c47be52b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1c1de851-66d3-424d-a3df-c39268a51a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_losing_dict(df, column):\n",
    "    \n",
    "    '''\n",
    "    building a dictionary for the averages by season by team for when they \n",
    "    were the losing team: \n",
    "    \n",
    "    Note - it is possible for a team to go undefeated, we will need to \n",
    "    figure out how to deal with that should we have an undefeated regular season\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    \n",
    "    features =['WScore', 'LScore', 'WFGM', 'WFGA', 'WFGM3','WFGA3', 'WFTM', 'WFTA', 'WOR',\n",
    "                'WTO',\n",
    "            'WDR', 'WAst', 'WStl', 'WBlk', 'WPF', 'LFGM', 'LFGA', 'LFGM3', 'LFGA3',\n",
    "       'LFTM', 'LFTA', 'LOR', 'LDR', 'LAst', 'LTO', 'LStl', 'LBlk', 'LPF', ]\n",
    "    \n",
    "    \n",
    "    rename = ['LScore', 'WScore', 'LFGM', 'LFGA', 'LFGM3','LFGA3', 'LFTM', 'LFTA', 'LOR',\n",
    "                'LTO',\n",
    "            'LDR', 'LAst', 'LStl', 'LBlk', 'LPF', 'WFGM', 'WFGA', 'WFGM3', 'WFGA3',\n",
    "       'WFTM', 'WFTA', 'WOR', 'WDR', 'WAst', 'WTO', 'WStl', 'WBlk', 'WPF', ]\n",
    "    \n",
    "        \n",
    "    w = df.groupby(by=[column])\n",
    "    w1 = pd.DataFrame(w)\n",
    "    teams = {}\n",
    "    \n",
    "    for i in tqdm(range(len(w1))):\n",
    "        #print(i)\n",
    "        ws = w1[1][i].groupby(\"Season\")\n",
    "        ws = pd.DataFrame(ws)\n",
    "        seasons = {}\n",
    "        for j in range(len(ws)):\n",
    "            season = ws[0][j]\n",
    "            data = ws[1][j][features]\n",
    "            data.columns = rename\n",
    "            #vals = dict(data.sum(axis = 0)/len(data))\n",
    "            vals = dict(data.sum(axis = 0))\n",
    "            vals.update({'season_losses': len(ws[1][j])})\n",
    "\n",
    "            seasons.update({ws[0][j]:  vals})\n",
    "        \n",
    "        teams.update({w1[0][i]:seasons})\n",
    "    return teams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "25e19ca6-7109-4227-ab9c-118cd82cdacd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████| 363/363 [00:10<00:00, 34.77it/s]\n"
     ]
    }
   ],
   "source": [
    "l_feats = get_losing_dict(reg_deets, \"LTeamID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a483e706-7cc1-4247-bf25-1f299c260ab5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "37bbe3ac-87b5-4468-875d-ce2520b83675",
   "metadata": {},
   "source": [
    "#### Offensive Efficiency\n",
    "\n",
    "Calculate the number of total number of possessions for your team using the formula: field goals attempted - offensive rebounds + turnovers + (0.4 x free throws attempted) = total number of possessions for the season. This works because a possession can end only in one of three ways: an attempted field goal, a turnover or a free throw, with an offensive rebound negating additional field goal attempts.\n",
    "\n",
    "Divide the team's total points scored for the season by the possessions you calculated in Step 1. For example, 938 total points scored divided by 998 total possessions gives your team 0.94 points scored per possession. Numbers above 1.0 are generally considered good.\n",
    "\n",
    "Convert the offensive PPP number to an efficiency rating by simply multipling by 100. So 0.94 points scored per possession becomes an offensive efficiency rating of 94.\n",
    "\n",
    "#### Defensive Efficiency\n",
    "\n",
    "Use the formula field goals attempted - offensive rebounds + turnovers + (0.4 x free throws attempted) = total number of possessions for the season to calculate total team possessions.\n",
    "\n",
    "Divide the total number of points allowed by your team by the possession total you calculated in Step 1. For example, 1009 total points allowed divided by 998 total possessions gives your team 1.01 points allowed per possession. The opposite is true for defensive PPP: Above 1.0 is bad; below 1.0 is considered good.\n",
    "\n",
    "Convert defensive points per possession to a defensive efficiency rating by multiplying by 100. So 1.01 points allowed per possession becomes a defensive efficiency rating of 101.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3b2981-89f4-48e9-8dc4-acae9ddd4259",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1fd46f41-8033-4444-b346-7c34870ab020",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine2(w_dict, l_dict):\n",
    "    '''\n",
    "    Parameters: winnign dictioanry, losing dictionary \n",
    "    \n",
    "    combining the dictioanries for games won and games lost \n",
    "    for the regular season for teams. we will then combine them \n",
    "    and get averages for each statistical category and compute \n",
    "    things like offensive and defensive efficiency. \n",
    "    \n",
    "    -- NOTE: if we use one of the efficiency metrics, we will not\n",
    "    be able to use the features that are inputs to that column. \n",
    "    because as given, the efficiency is simply a linear combination\n",
    "    of other features, this will make our matrix non-invertable as our \n",
    "    columns will not be independent, so we will have to choose to either \n",
    "    keep all of the features seperate or combine and drop some \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    \n",
    "    total = {}\n",
    "    first = reg_deets.WTeamID.min()\n",
    "    last = reg_deets.WTeamID.max()+1\n",
    "    \n",
    "    features = ['WScore', 'LScore', 'WFGM', 'WFGA', 'WFGM3','WFGA3', 'WFTM', 'WFTA', 'WOR',\n",
    "                'WTO',\n",
    "            'WDR', 'WAst', 'WStl', 'WBlk', 'WPF', 'LFGM', 'LFGA', 'LFGM3', 'LFGA3',\n",
    "       'LFTM', 'LFTA', 'LOR', 'LDR', 'LAst', 'LTO', 'LStl', 'LBlk', 'LPF',]\n",
    "    \n",
    "    for i in tqdm(range(first, last)):\n",
    "        try:\n",
    "            \n",
    "            season = {}\n",
    "            f_season = pd.DataFrame(w_dict[1102]).T.index.min()\n",
    "            l_season = pd.DataFrame(w_dict[1102]).T.index.max()+1\n",
    "            w_l = {}\n",
    "            for j in range(f_season, l_season):\n",
    "                w_df = pd.DataFrame(w_dict[i]).T\n",
    "                l_df = pd.DataFrame(l_dict[i]).T\n",
    "\n",
    "                w_df_r = w_df.reset_index()\n",
    "                l_df_r = l_df.reset_index()\n",
    "\n",
    "\n",
    "\n",
    "                temp  = pd.concat([w_df_r[w_df_r['index'] == j],l_df_r[l_df_r['index'] == j]])\n",
    "                temp = temp.fillna(0)\n",
    "\n",
    "                games = np.sum(temp['season_losses'])+np.sum(temp['season_wins'])\n",
    "                \n",
    "                #temp[features].sum(axis = 0)/games\n",
    "                season_avg = dict(temp[features].sum(axis = 0)/games)\n",
    "                \n",
    "                w_l.update({'num_loss': np.sum(temp['season_losses'])})\n",
    "                w_l.update({'num_wins': np.sum(temp['season_wins'])})\n",
    "\n",
    "                season_avg = season_avg|w_l\n",
    "                \n",
    "                \n",
    "                new_key = ['Score', 'Op_score', 'FGM', 'FGA', 'FGM3', 'FGA3', 'FTM', 'FTA', 'OR', 'TO', \n",
    "               'DR', 'Ast', 'Stl', 'Blk', 'PF', 'Op_FGM', 'Op_FGA', 'Op_FGM3', 'Op_FGA3', \n",
    "               'Op_FTM', 'Op_FTA', 'Op_OR', 'Op_DR', 'Op_Ast', 'Op_TO', 'Op_Stl', \n",
    "               'Op_Blk', 'Op_PF', 'num_loss', 'num_wins', 'conf']\n",
    "                \n",
    "                vals = list(season_avg.values())\n",
    "                #print(vals)\n",
    "                new = dict(zip(new_key, vals))\n",
    "                off_eff = (new['FGA'] - new['OR']+new['Op_TO']+(0.47*new['FTA']))/(new['Score'])*100\n",
    "                new.update({'off_eff':off_eff})\n",
    "                \n",
    "                def_eff = (new['Op_FGA'] - new['Op_OR']+new['TO']+(0.47*new['Op_FTA']))/(new['Op_score'])*100\n",
    "                new.update({'def_eff':def_eff})\n",
    "                \n",
    "                equalizer = (new['FGM3']/new['FGA3'])*100\n",
    "                new.update({'three_perc':equalizer})\n",
    "                \n",
    "                d_equal = (new['Op_FGM3']/new['Op_FGA3'])*100\n",
    "                new.update({'d_three_perc':d_equal})\n",
    "                \n",
    "                #print(vals)\n",
    "                #season.update({j:  season_avg})\n",
    "                season.update({j:  new})\n",
    "\n",
    "                \n",
    "                \n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "        total.update({i:season})\n",
    "    return total "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6b4db5eb-5e2e-453d-86d5-e799035c93f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████| 372/372 [01:07<00:00,  5.49it/s]\n"
     ]
    }
   ],
   "source": [
    "total_dict = combine2(w_feats, l_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "94461a5d-1e15-49fc-b187-d0b18a4f472f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_conf_dict(df):\n",
    "    \n",
    "    '''\n",
    "    Many of the tournament titles are played on \"Selection Sunday\"\n",
    "    the day the tournament is seeded.  others are played throughout\n",
    "    \"Championship Week\". We will need to find a way to group by \n",
    "    conference and select the winner on the last day a game was played \n",
    "    for a particular conference \n",
    "\n",
    "    '''\n",
    "    \n",
    "    conf_champ = conf_tour.groupby([\"Season\", \"ConfAbbrev\"])[\"DayNum\"].max() \n",
    "    conf_champ\n",
    "    cdf = pd.DataFrame(conf_champ).reset_index()\n",
    "    cdf2 = pd.merge(left = cdf, right = conf_tour, on = ['Season', 'ConfAbbrev', 'DayNum'])\n",
    "    cdf2 = cdf2.drop(columns = ['LTeamID'])\n",
    "    \n",
    "    teams = {}\n",
    "    cdf2 = pd.DataFrame(cdf2.groupby('Season'))\n",
    "    for i in tqdm(range(len(cdf2))):\n",
    "        #print(i)\n",
    "        season = cdf2[1][i]\n",
    "        year = cdf2[0][i]\n",
    "        champs = dict(zip(cdf2[1][i]['WTeamID'],cdf2[1][i]['ConfAbbrev']))\n",
    "        teams.update({year: champs})\n",
    "\n",
    "    return teams\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5192b65e-8f5a-4a97-8e7a-50c09e571031",
   "metadata": {},
   "outputs": [],
   "source": [
    "### make a teams conf dict similar to above "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "72fa715b-6c24-46bb-9569-6a1abd3f9633",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████| 22/22 [00:00<00:00, 7823.87it/s]\n"
     ]
    }
   ],
   "source": [
    "conf_champs = get_conf_dict(conf_tour)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53bc0aa6-7430-4a58-9796-78484d09064d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9919b114-5d64-4488-ad24-34d8c4022976",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_conf_affiliation_dict(df):\n",
    "    '''\n",
    "    Parameters: Regular deets 2 which has been merged with the \n",
    "    conference tournament data to get the conference afiiliatoin \n",
    "    by season by team\n",
    "    As teams can  move conferences from year to year, it is unclear \n",
    "    if the marginal change affects teams, but we wanted to gather the \n",
    "    confernece affect and then group them into power, mid major and \n",
    "    low major levels to be done later \n",
    "    \n",
    "    \n",
    "    '''\n",
    "    reg_deets2\n",
    "    t1 = pd.DataFrame(df.groupby(\"Season\"))\n",
    "    cf = {}\n",
    "    for i in range(len(t1)):\n",
    "        \n",
    "        d1 = dict(zip(t1[1][i]['WTeamID'], t1[1][i]['WTeam_Conf']))\n",
    "        d2 = dict(zip(t1[1][i]['LTeamID'], t1[1][i]['LTeam_Conf']))\n",
    "        d3 =d1|d2\n",
    "        cf.update({t1[0][i]:d3})\n",
    "    \n",
    "        \n",
    "    return cf\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7b3f6055-57b8-44e9-acd8-bc0bef2198d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_affil = get_conf_affiliation_dict(reg_deets2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff3b1df-188c-4960-a04c-f30096d0afc5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038c6784-69db-41d7-aceb-2a51a6e555ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "86ab1b57-f01a-48df-b60e-5fa7e6ca9683",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_power_seeds_dict(df):\n",
    "    \n",
    "    #first add team id to TeamNamein power seeds \n",
    "    \n",
    "    teams2 = teams_df[['TeamID', 'TeamName']]\n",
    "    \n",
    "    df2 = pd.merge(left = df, right = teams2, on = 'TeamName') \n",
    "    \n",
    "    df2.to_csv('../data/power_test.csv')\n",
    "    ps1 = pd.DataFrame(df2.groupby(by = \"Season\"))\n",
    "    empty = {}\n",
    "\n",
    "    for i in range(len(ps1)):\n",
    "        empty.update({ps1[0][i]: dict(zip(ps1[1][i]['TeamID'], ps1[1][i]['PowerSeed ']))})\n",
    "\n",
    "\n",
    "    return empty, df2, ps1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "7f18c88f-8c7d-4116-9e73-fc8bff2a04ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "psd, t2, pdf  = create_power_seeds_dict(power_seeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c853d29b-f785-4c37-aaca-849d678b3dfc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "babb7151-3c0d-4c84-ada5-1dea3cdcca93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TeamID</th>\n",
       "      <th>TeamName</th>\n",
       "      <th>FirstD1Season</th>\n",
       "      <th>LastD1Season</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>317</th>\n",
       "      <td>1418</td>\n",
       "      <td>Louisiana</td>\n",
       "      <td>1985</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     TeamID   TeamName  FirstD1Season  LastD1Season\n",
       "317    1418  Louisiana           1985          2022"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "teams_df[teams_df['TeamID'] == 1418]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "0d37922e-fd32-4ae5-9921-fb64045e75b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_power_seeds_tourney(df):\n",
    "    ''' takes in tourney_deets and gets L team and w team power seeds\n",
    "    \n",
    "    TIP: 134, 135 are play in days \n",
    "    '''\n",
    "    \n",
    "    \n",
    "    df = df[df['DayNum']  >= 137]\n",
    "    df = df.reset_index()\n",
    "    df = df.drop(columns =['index'])\n",
    "    \n",
    "    lteam = []\n",
    "    wteam = []\n",
    "    for i in tqdm(range(len(df))):\n",
    "        #print(i)\n",
    "        try: \n",
    "            lteam.append(psd[df['Season'][i]][df['LTeamID'][i]])\n",
    "            wteam.append(psd[df['Season'][i]][df['WTeamID'][i]])\n",
    "            #pass\n",
    "        except:  \n",
    "            print(df['Season'][i])\n",
    "            print(i)\n",
    "            print(df['LTeamID'][i])\n",
    "            print(\" \")\n",
    "            #wteam.append(\"error\")\n",
    "            #lteam.append('error')\n",
    "            #pass\n",
    "    df['lteam_seed'] = lteam\n",
    "    df['wteam_seed'] = wteam\n",
    "    \n",
    "    df = df.reset_index()\n",
    "    df = df.drop(columns = ['index'])\n",
    "    return df \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "a8298eda-0684-4b1c-ad10-e2de677e5e93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████| 861/861 [00:00<00:00, 34471.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2013\n",
      "471\n",
      "1107\n",
      " \n",
      "2013\n",
      "489\n",
      "1433\n",
      " \n",
      "2014\n",
      "527\n",
      "1433\n",
      " \n",
      "2014\n",
      "532\n",
      "1142\n",
      " \n",
      "2015\n",
      "573\n",
      "1107\n",
      " \n",
      "2019\n",
      "756\n",
      "1280\n",
      " \n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Length of values (855) does not match length of index (861)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [192]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m tour2 \u001b[38;5;241m=\u001b[39m \u001b[43mget_power_seeds_tourney\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtourney_deets\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [191]\u001b[0m, in \u001b[0;36mget_power_seeds_tourney\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     25\u001b[0m         \u001b[38;5;66;03m#wteam.append(\"error\")\u001b[39;00m\n\u001b[1;32m     26\u001b[0m         \u001b[38;5;66;03m#lteam.append('error')\u001b[39;00m\n\u001b[1;32m     27\u001b[0m         \u001b[38;5;66;03m#pass\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlteam_seed\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m lteam\n\u001b[1;32m     29\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwteam_seed\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m wteam\n\u001b[1;32m     31\u001b[0m df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mreset_index()\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/frame.py:3655\u001b[0m, in \u001b[0;36mDataFrame.__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   3652\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setitem_array([key], value)\n\u001b[1;32m   3653\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3654\u001b[0m     \u001b[38;5;66;03m# set column\u001b[39;00m\n\u001b[0;32m-> 3655\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_set_item\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/frame.py:3832\u001b[0m, in \u001b[0;36mDataFrame._set_item\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   3822\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_set_item\u001b[39m(\u001b[38;5;28mself\u001b[39m, key, value) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3823\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   3824\u001b[0m \u001b[38;5;124;03m    Add series to DataFrame in specified column.\u001b[39;00m\n\u001b[1;32m   3825\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3830\u001b[0m \u001b[38;5;124;03m    ensure homogeneity.\u001b[39;00m\n\u001b[1;32m   3831\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 3832\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sanitize_column\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3834\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   3835\u001b[0m         key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\n\u001b[1;32m   3836\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m value\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   3837\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_extension_array_dtype(value)\n\u001b[1;32m   3838\u001b[0m     ):\n\u001b[1;32m   3839\u001b[0m         \u001b[38;5;66;03m# broadcast across multiple columns if necessary\u001b[39;00m\n\u001b[1;32m   3840\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mis_unique \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns, MultiIndex):\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/frame.py:4535\u001b[0m, in \u001b[0;36mDataFrame._sanitize_column\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m   4532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _reindex_for_setitem(value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex)\n\u001b[1;32m   4534\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_list_like(value):\n\u001b[0;32m-> 4535\u001b[0m     \u001b[43mcom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequire_length_match\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4536\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m sanitize_array(value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, allow_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/common.py:557\u001b[0m, in \u001b[0;36mrequire_length_match\u001b[0;34m(data, index)\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    554\u001b[0m \u001b[38;5;124;03mCheck the length of data matches the length of the index.\u001b[39;00m\n\u001b[1;32m    555\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    556\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(index):\n\u001b[0;32m--> 557\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    558\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLength of values \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    559\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(data)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    560\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdoes not match length of index \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    561\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(index)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    562\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Length of values (855) does not match length of index (861)"
     ]
    }
   ],
   "source": [
    "tour2 = get_power_seeds_tourney(tourney_deets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "ddcd9ba2-9d06-48fe-b04d-1d417fd59c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_l_seed(df):    \n",
    "    test = []\n",
    "    for i in range(len(df)):\n",
    "        #try: \n",
    "        test.append(psd[df['Season'][i]][df['LTeamID'][i]])\n",
    "        #except:\n",
    "            #test.append('error')\n",
    "    df['lteam_seed'] = test\n",
    "    \n",
    "    return df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772cf356-a0bf-4b07-96e5-3f3a0ba21ad6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "573a6054-e173-4433-af94-60a4fa2a38a8",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/indexes/base.py:3621\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3620\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3621\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3622\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/_libs/index.pyx:136\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/_libs/index.pyx:163\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:2131\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:2140\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 0",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[0;32mIn [170]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m tour2 \u001b[38;5;241m=\u001b[39m \u001b[43mget_l_seed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtourney_deets2\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [161]\u001b[0m, in \u001b[0;36mget_l_seed\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m      2\u001b[0m test \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(df)):\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;66;03m#try: \u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m     test\u001b[38;5;241m.\u001b[39mappend(psd[\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mSeason\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m][df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLTeamID\u001b[39m\u001b[38;5;124m'\u001b[39m][i]])\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m#except:\u001b[39;00m\n\u001b[1;32m      7\u001b[0m         \u001b[38;5;66;03m#test.append('error')\u001b[39;00m\n\u001b[1;32m      8\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlteam_seed\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m test\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/series.py:958\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    955\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[key]\n\u001b[1;32m    957\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m key_is_scalar:\n\u001b[0;32m--> 958\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    960\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_hashable(key):\n\u001b[1;32m    961\u001b[0m     \u001b[38;5;66;03m# Otherwise index.get_value will raise InvalidIndexError\u001b[39;00m\n\u001b[1;32m    962\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    963\u001b[0m         \u001b[38;5;66;03m# For labels that don't resolve as scalars like tuples and frozensets\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/series.py:1069\u001b[0m, in \u001b[0;36mSeries._get_value\u001b[0;34m(self, label, takeable)\u001b[0m\n\u001b[1;32m   1066\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[label]\n\u001b[1;32m   1068\u001b[0m \u001b[38;5;66;03m# Similar to Index.get_value, but we do not fall back to positional\u001b[39;00m\n\u001b[0;32m-> 1069\u001b[0m loc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1070\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39m_get_values_for_loc(\u001b[38;5;28mself\u001b[39m, loc, label)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/indexes/base.py:3623\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3621\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3622\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m-> 3623\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3624\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3625\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3626\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3627\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3628\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "tour2 = get_l_seed(tourney_deets2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed6f1a5-bedd-4826-8b21-05aab3473947",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25b8bea-4120-4fbf-9d11-02964ca2bfe2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac4eb04f-e735-4a59-b45e-c5097870d866",
   "metadata": {},
   "outputs": [],
   "source": [
    "tour3 = tour2[tour2['wteam_seed'] != 'come back to']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa20aba-d973-4136-86d1-ed6bb1bfbf3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_data(df):\n",
    "    df = df.reset_index()\n",
    "    ''' \n",
    "    takes in tourney_deets df and \n",
    "    renames columns based on seeding of low and high seeds\n",
    "    instead of winning and losing teams, this way our \n",
    "    model will not just memorize the order of columns and that \n",
    "    will tell the computer who won. And the games we have haven't \n",
    "    happened yet so like it doesnt even make sense to run that way lol \n",
    "    \n",
    "    \n",
    "    ''' \n",
    "    \n",
    "    fav = {'Season':'Season', 'DayNum':'DayNum',\n",
    "           \n",
    "           'WTeamID':'HSTeamID', \n",
    "           'LTeamID':'LSTeamID', 'WLoc':'WLoc', 'NumOT':'NumOT', \n",
    "       \n",
    "       'WFGM':'HSFGM', 'WFGA':'HSFGA', 'WFGM3':'HSFGM3', \n",
    "       'WFGA3':'HSFGA3', 'WFTM':'HSFTM', 'WFTA':'HSFTA', \n",
    "       'WOR':'HSOR', 'WDR':'HSDR', 'WAst':'HSAst', \n",
    "       'WTO':'HSTO', 'WStl':'HSStl', 'WBlk':'HSBlk', \n",
    "       'WPF':'HSPF', \"WScore\":\"HS_Score\", \n",
    "       \n",
    "       'LFGM':'LSFGM', 'LFGA':'LSFGA', 'LFGM3':'LSFGM3', \n",
    "       'LFGA3':'LSFGA3', 'LFTM':'LSFTM', 'LFTA':'LSFTA', \n",
    "       'LOR':'LSOR', 'LDR':'LSDR', 'LAst':'LSAst', \n",
    "       'LStl':'LSStl', 'LBlk':'LSBlk', 'LPF':'LSPF', \n",
    "        'LTO':'LSTO', \"LScore\":\"LS_Score\", \n",
    "       \n",
    "       \n",
    "       \n",
    "       'lteam_seed':'ls_seed', 'wteam_seed':'ws_seed'}\n",
    "    \n",
    "    up = {'Season':'Season', 'DayNum':'DayNum', 'WTeamID':'LSTeamID', \n",
    "       'LTeamID':'HSTeamID', 'WLoc':'WLoc', 'NumOT':'NumOT', \n",
    "      \n",
    "       'WFGM':'LSFGM', 'WFGA':'LSFGA', 'WFGM3':'LSFGM3', \n",
    "       'WFGA3':'LSFGA3', 'WFTM':'LSFTM', 'WFTA':'LSFTA', \n",
    "       'WOR':'LSOR', 'WDR':'LSDR', 'WAst':'LSAst', \n",
    "       'WTO':'LSTO', 'WStl':'LSStl', 'WBlk':'LSBlk', \n",
    "       'WPF':'LSPF', \"WScore\":\"LS_Score\", \n",
    "       \n",
    "       'LFGM':'HSFGM', 'LFGA':'HSFGA', 'LFGM3':'HSFGM3', \n",
    "       'LFGA3':'HSFGA3', 'LFTM':'HSFTM', 'LFTA':'HSFTA', \n",
    "       'LOR':'HSOR', 'LDR':'HSDR', 'LAst':'HSAst', \n",
    "       'LStl':'HSStl', 'LBlk':'HSBlk', 'LPF':'HSPF',\n",
    "       'LTO':'HSTO', \"LScore\":\"HS_Score\", \n",
    "       \n",
    "       'lteam_seed':'ls_seed', 'wteam_seed':'ws_seed'}\n",
    "    \n",
    "    cols = list(fav.values())\n",
    "    empty = pd.DataFrame(columns = cols)\n",
    "    \n",
    "    for i in range(len(df)):\n",
    "  \n",
    "        if df['wteam_seed'][i] < df['lteam_seed'][i]:\n",
    "            #then the favorite won, apply favorite dictionary \n",
    "            \n",
    "            row = df.iloc[[i]]\n",
    "            row = row.rename(columns = fav)\n",
    "            empty = pd.concat([empty, row])\n",
    "        else:\n",
    "            #apply upset_dictioanry \n",
    "            row = df.iloc[[i]]\n",
    "            row = row.rename(columns = up)\n",
    "            empty = pd.concat([empty, row])\n",
    "            \n",
    "    \n",
    "    empty = empty.drop(columns = ['index'])\n",
    "    return empty \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca501fe2-186d-4f19-ae0c-78af823d2366",
   "metadata": {},
   "outputs": [],
   "source": [
    "tour4 = prep_data(tour3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46d74f9-d809-41c6-a912-46dbbb2a7007",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9509f2a7-a01c-45d6-956c-a8d82fc1189e",
   "metadata": {},
   "source": [
    "#### TO DO \n",
    "\n",
    "figure out feature set \n",
    " might want to have Neural net vs supervized... research here -- just do both, use keras pre built so dont have to stress too hard, \n",
    "\n",
    "create output --> input files for progressive rounds \n",
    "\n",
    "get with noah wednesday night\n",
    "\n",
    "create power seeds for 2023 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb2b2116-7255-48a6-9a44-b7ed3f6c6551",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_historical_tourney_perc(df):\n",
    "    ''' so im at this problem where I cant really use team ID as a dummy column \n",
    "    if I want to do a supervized learning technique, the problem being that \n",
    "    I would have a massive amount of columns, which would then make inverting \n",
    "    it very difficult, I would get a massive number of empty columns, so my data\n",
    "    would get very sparce very quickly. \n",
    "    \n",
    "    the reason I wanted to use team id would be for the program to have some sort of \n",
    "    way to recognize teams that do well across the tournament, Duke, Kentucky, and \n",
    "    teams of that nature come to mind. \n",
    "    \n",
    "    Going to build a dictionary with historical win percentages in the tournament. \n",
    "    \n",
    "    we will start from 1985, and go until 2003, and that will be the win percentage for \n",
    "    teams going into the 2003 tournament. Then we will have each year from 2003 onwards \n",
    "    will be its own year, and will be updated according to the wins and losses of the previous \n",
    "    tournament.\n",
    "    \n",
    "    this way we do not bias the results by including games that have already happened \n",
    "    for historical tournaments. \n",
    "    ''' \n",
    "    \n",
    "    # first read in the compact results for the NCAA tournament going back to 1985 \n",
    "    \n",
    "    keys = list(range(2003, 2022))\n",
    "    #print(keys)\n",
    "    t_perc = {}\n",
    "    for i in range(len(keys)):\n",
    "        #initialize dict \n",
    "        season = {}\n",
    "        #find historical seasons \n",
    "        c_tour = df[df['Season'] < keys[i]]\n",
    "        \n",
    "        year = keys[i]\n",
    "        \n",
    "        wins = c_tour['WTeamID'].value_counts().reset_index()\n",
    "        losses = c_tour['LTeamID'].value_counts().reset_index()\n",
    "        \n",
    "        w_l = pd.merge(left = wins, right = losses, on = 'index')\n",
    "        w_l['total_games'] = w_l['WTeamID']+w_l['LTeamID']\n",
    "        \n",
    "        w_l['win_perc'] = w_l['WTeamID']/w_l['total_games']\n",
    "        \n",
    "        w_l['Season'] = year\n",
    "        year_dict = dict(zip(w_l['index'], w_l['win_perc']))\n",
    "        \n",
    "        t_perc.update({year:year_dict})\n",
    "        \n",
    "        \n",
    "    return t_perc      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc4b719-2ea4-42eb-b080-dce216fcf7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tw = get_historical_tourney_perc(compact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776c3b73-0dae-4473-8d53-46ea32d3d067",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8852794e-3d7d-41bb-a601-d395bc534f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create page rank data \n",
    "def make_page_data(df):\n",
    "    \"\"\"\n",
    "    Parameters: takes in regular season compact details, \n",
    "    \n",
    "    we want to calculate page rank: \n",
    "    \n",
    "    Page rank is a google algorithm which is a network theory \n",
    "    result basically giving you the degree centrality for the \n",
    "    different nodes in the network. \n",
    "    \n",
    "    here our nodes are teams, and the edges are if they played. \n",
    "    we want to know which teams beat which teams who beat which \n",
    "    teams, there by we can get some sort of degree centrality for \n",
    "    the team that beat the most powerful teams... \n",
    "    \n",
    "    we need to get the yearly winners and loosers for the page rank\n",
    "    structure we have from a homework assignment i did back in like \n",
    "    2018. So we get the by year regular season data, store that in \n",
    "    individual sheets which will then be fed into our page rank \n",
    "    algorithm to give us a by year by team page rank value\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    for i in range(2003, 2023):\n",
    "        season = df[df['Season'] == i]\n",
    "        \n",
    "        data = season[['WTeamID', \"LTeamID\"]]\n",
    "        data = data.rename(columns = {\"WTeamID\":'Winner', 'LTeamID':'Loser'})\n",
    "        data.to_csv('../data/page_rank_data/'+str(i)+\"_page_data.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa0bb29-a4ab-4f5f-86d6-dc17dac9e860",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_page_data(reg_deets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c203dc-8f5b-4592-b8e4-d7c324d7e3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problems 1-2\n",
    "# Shout out to Dr Evans for making me do this homework lol \n",
    "\n",
    "class DiGraph:\n",
    "    \"\"\"A class for representing directed graphs via their adjacency matrices.\n",
    "\n",
    "    Attributes:\n",
    "        (fill this out after completing DiGraph.__init__().)\n",
    "    \"\"\"\n",
    "    # Problem 1\n",
    "    def __init__(self, A, labels=None):\n",
    "        \"\"\"Modify A so that there are no sinks in the corresponding graph,\n",
    "        then calculate Ahat. Save Ahat and the labels as attributes.\n",
    "\n",
    "        Parameters:\n",
    "            A ((n,n) ndarray): the adjacency matrix of a directed graph.\n",
    "                A[i,j] is the weight of the edge from node j to node i.\n",
    "            labels (list(str)): labels for the n nodes in the graph.\n",
    "                If None, defaults to [0, 1, ..., n-1].\n",
    "        \"\"\"\n",
    "        for i in range(len(A)):\n",
    "            if(np.allclose(A[:,i],np.zeros(len(A[0])))):\n",
    "                #remove sink\n",
    "                A[:,i] = 1\n",
    "        #create ahat\n",
    "        self.Ahat =  A / A.sum(axis=0)\n",
    "        #get length of A\n",
    "        self.n = len(A)\n",
    "        #if no labels\n",
    "        if labels == None:\n",
    "            #Set standard\n",
    "            self.labels = np.arange(0,len(A))\n",
    "        else:\n",
    "            #otherwise use labels\n",
    "            self.labels = labels\n",
    "            #throw an error if it dosn't match\n",
    "        if len(A) != len(self.labels):\n",
    "            raise ValueError(\"Number of labels is not equal to the number of nodes\")\n",
    "\n",
    "\n",
    "    # Problem 2\n",
    "    def linsolve(self, epsilon=0.85):\n",
    "        \"\"\"Compute the PageRank vector using the linear system method.\n",
    "\n",
    "        Parameters:\n",
    "            epsilon (float): the damping factor, between 0 and 1.\n",
    "\n",
    "        Returns:\n",
    "            dict(str -> float): A dictionary mapping labels to PageRank values.\n",
    "        \"\"\"\n",
    "        #find values\n",
    "        p = la.solve(np.identity(self.n)-epsilon*self.Ahat,(1-epsilon)*np.ones(self.n)/self.n)\n",
    "        #make a dictionary\n",
    "        dict = {self.labels[i]:p[i] for i in range(self.n)}\n",
    "        return dict\n",
    "\n",
    "    # Problem 2\n",
    "    def eigensolve(self, epsilon=0.85):\n",
    "        \"\"\"Compute the PageRank vector using the eigenvalue method.\n",
    "        Normalize the resulting eigenvector so its entries sum to 1.\n",
    "\n",
    "        Parameters:\n",
    "            epsilon (float): the damping factor, between 0 and 1.\n",
    "\n",
    "        Return:\n",
    "            dict(str -> float): A dictionary mapping labels to PageRank values.\n",
    "        \"\"\"\n",
    "        #make B\n",
    "        B = epsilon*self.Ahat+(1-epsilon)/self.n*np.ones((self.n,self.n))\n",
    "        #get eig stuff\n",
    "        eigvals, eigvects = la.eig(B)\n",
    "        #get the eig vector for the largest value\n",
    "        p = eigvects[:,0].real\n",
    "        #normalize it\n",
    "        p = p/p.sum()\n",
    "        #make a dictionary\n",
    "        D = {self.labels[i]:p[i] for i in range(self.n)}\n",
    "        #return the dictionary\n",
    "        return D\n",
    "\n",
    "    # Problem 2\n",
    "    def itersolve(self, epsilon=0.85, maxiter=100, tol=1e-12):\n",
    "        \"\"\"Compute the PageRank vector using the iterative method.\n",
    "\n",
    "        Parameters:\n",
    "            epsilon (float): the damping factor, between 0 and 1.\n",
    "            maxiter (int): the maximum number of iterations to compute.\n",
    "            tol (float): the convergence tolerance.\n",
    "\n",
    "        Return:\n",
    "            dict(str -> float): A dictionary mapping labels to PageRank values.\n",
    "        \"\"\"\n",
    "        t = 0\n",
    "        #set p\n",
    "        p = [1/self.n for n in range(self.n)]\n",
    "        #while loop\n",
    "        while t < maxiter:\n",
    "            t += 1\n",
    "            #get the next p\n",
    "            pt = epsilon*self.Ahat@p + (1 - epsilon)*np.ones(self.n)/self.n\n",
    "            #stop if it meets the stopping condition\n",
    "            if(la.norm(pt - p, ord=1) < tol):\n",
    "                break\n",
    "            #otherwise reset\n",
    "            p = pt\n",
    "        #create the dictionary\n",
    "        D = {self.labels[i]:pt[i] for i in range(self.n)}\n",
    "        #return the dicitonary\n",
    "        return D\n",
    "\n",
    "# Problem 3\n",
    "def get_ranks(d):\n",
    "    \"\"\"Construct a sorted list of labels based on the PageRank vector.\n",
    "\n",
    "    Parameters:\n",
    "        d (dict(str -> float)): a dictionary mapping labels to PageRank values.\n",
    "\n",
    "    Returns:\n",
    "        (list) the keys of d, sorted by PageRank value from greatest to least.\n",
    "    \"\"\"\n",
    "    #get the keys\n",
    "    keys = np.array(list(d.keys()))\n",
    "    #get the values\n",
    "    values = list(d.values())\n",
    "    #create a mask\n",
    "    mask = np.array(np.argsort(values)[::-1])\n",
    "    #return the stuff\n",
    "    return list(keys[mask])\n",
    "\n",
    "# Problem 5\n",
    "def rank_ncaa_teams(filename, epsilon=0.85):\n",
    "    \"\"\"Read the specified file and construct a graph where node j points to\n",
    "    node i with weight w if team j was defeated by team i in w games. Use the\n",
    "    DiGraph class and its itersolve() method to compute the PageRank values of\n",
    "    the teams, then rank them with get_ranks().\n",
    "\n",
    "    Each line of the file has the format\n",
    "        A,B\n",
    "    meaning team A defeated team B.\n",
    "\n",
    "    Parameters:\n",
    "        filename (str): the name of the data file to read.\n",
    "        epsilon (float): the damping factor, between 0 and 1.\n",
    "\n",
    "    Returns:\n",
    "        (list(str)): The ranked list of team names.\n",
    "    \"\"\"\n",
    "    labels = []\n",
    "    with open(filename,'r') as infile:\n",
    "        content = infile.read().strip()\n",
    "    #print(content)\n",
    "    labels = sorted(set(content.replace('\\n',',').split(',')))\n",
    "    labels.remove(\"Loser\")\n",
    "    labels.remove(\"Winner\")\n",
    "    #init dictionary\n",
    "    Dict = {team: i for i, team in enumerate(labels)}\n",
    "    #init adjacency matrix\n",
    "    A = np.zeros((len(labels), len(labels)))\n",
    "    #get the information from the lines\n",
    "    for line in content.split('\\n')[1:]:\n",
    "        teams = line.split(',')\n",
    "        row = Dict[teams[0]]\n",
    "        column = Dict[teams[1]]\n",
    "        A[row][column] += 1\n",
    "    D = DiGraph(A, labels=labels)\n",
    "    #get the page rank\n",
    "    PageRank_ = D.itersolve(epsilon = epsilon)\n",
    "    #return the stuff\n",
    "    #print(PageRank_)\n",
    "    return PageRank_, get_ranks(PageRank_)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7c5e90-91f0-4b83-997c-db65977344e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_page_rank():\n",
    "    '''\n",
    "    we take the page rank algorithm from above \n",
    "    which takes in the page rank data that we have saved \n",
    "    previously. \n",
    "    \n",
    "    it will then return a dictionary of seasons \n",
    "    which will have a dicitonary of teams and their \n",
    "    degree centrality (page rank), which we will then \n",
    "    use as a feature for our algorithm later on \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    page_data = {}\n",
    "    teams_rank = {}\n",
    "    path = '../data/page_rank_data/'\n",
    "    for i in range(2003, 2023):\n",
    "        full_path = path+str(i)+\"_page_data.csv\"\n",
    "        #year_data = pd.read_csv(full_path)\n",
    "        \n",
    "        year_page, ranked = rank_ncaa_teams(full_path)\n",
    "        \n",
    "        page_data.update({i:year_page})\n",
    "        teams_rank.update({i:ranked})\n",
    "        \n",
    "    return page_data, teams_rank\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8230dea6-f7f6-491a-b0e6-9299c511b7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "page_rank, teams_rank = get_page_rank()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bbb9c51-991e-48b2-be97-93f55d18a88d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33469758-4085-4899-badc-92665015f6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_round(df):\n",
    "    '''\n",
    "    takes in tournament data, assigns a categorical \n",
    "    variable based on DayNum for the round \n",
    "    \n",
    "    DayNum=136 or 137 (Thu/Fri) - Round 1, to bring the tournament field from 64 teams to 32 teams\n",
    "    DayNum=138 or 139 (Sat/Sun) - Round 2, to bring the tournament field from 32 teams to 16 teams\n",
    "    DayNum=143 or 144 (Thu/Fri) - Round 3, otherwise known as \"Sweet Sixteen\", to bring the tournament field from 16 teams to 8 teams\n",
    "    DayNum=145 or 146 (Sat/Sun) - Round 4, otherwise known as \"Elite Eight\" or \"regional finals\", to bring the tournament field from 8 teams to 4 teams\n",
    "    DayNum=152 (Sat) - Round 5, otherwise known as \"Final Four\" or \"national semifinals\", to bring the tournament field from 4 teams to 2 teams\n",
    "    DayNum=154 (Mon) - Round 6, otherwise known as \"national final\" or \"national championship\", to bring the tournament field from 2 teams to 1 champion team\n",
    "    '''\n",
    "    \n",
    "    df['first_round'] = df['DayNum'].apply(lambda x: 1 if x in [136, 137] else 0)\n",
    "    df['second_round'] = df['DayNum'].apply(lambda x: 1 if x in [138, 139] else 0)\n",
    "    df['sweet_16'] = df['DayNum'].apply(lambda x: 1 if x in [143, 144] else 0)\n",
    "    df['elite_8'] = df['DayNum'].apply(lambda x: 1 if x in [145, 146] else 0)\n",
    "    df['final_four'] = df['DayNum'].apply(lambda x: 1 if x == 152 else 0)\n",
    "    df['championship'] = df['DayNum'].apply(lambda x: 1 if x == 154 else 0)\n",
    "    \n",
    "    return df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49289aca-0c47-4982-a245-985f54ec27b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tour4 = get_round(tour4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1dd6c29-029d-4497-9e1f-6ee287081e5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425c0434-d584-4e01-97c2-2c8d6e1d9a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reg_avgs(df):\n",
    "    \"\"\"\n",
    "    Parameters: Tournament details that has gone through the \n",
    "    cleaning and prepping phases highlighted above \n",
    "    \n",
    "    ADDITIONALLY: Although not a direct input, but another parameter is the total_dict \n",
    "    that we have build to have the regular season averages by team\n",
    "    by season, that will be used to fill the different features, \n",
    "    change those from the actual \"in game\" values to regular season \n",
    "    average values, and then we will add some more features here and \n",
    "    a few later on. \n",
    "    \n",
    "    Creating the features that we want for our modeling df --> \n",
    "    ready go! \n",
    "    \n",
    "    \"\"\"\n",
    "    df['HS_avg_score'] = np.zeros(len(df))\n",
    "    df['HS_avg_against'] = np.zeros(len(df))\n",
    "    \n",
    "    df['LS_avg_score'] = np.zeros(len(df))\n",
    "    df['LS_avg_against'] = np.zeros(len(df))\n",
    "    \n",
    "    df['HS_avg_def_eff'] = np.zeros(len(df))\n",
    "    df['LS_avg_def_eff'] = np.zeros(len(df))\n",
    "    \n",
    "    df['HS_wins'] = np.zeros(len(df))\n",
    "    df['HS_loss'] = np.zeros(len(df))\n",
    "    \n",
    "    df['LS_wins'] = np.zeros(len(df))\n",
    "    df['LS_loss'] = np.zeros(len(df))\n",
    "    \n",
    "    \n",
    "    for i in tqdm(range(len(df))):\n",
    "        #total_dict[tour5[\"HSTeamID\"][0]][tour5[\"Season\"][0]]\n",
    "        df['HS_avg_score'][i] = total_dict[df[\"HSTeamID\"][i]][df[\"Season\"][i]]['Score']\n",
    "        df['HS_avg_against'][i] = total_dict[df[\"HSTeamID\"][i]][df[\"Season\"][i]]['Op_score']\n",
    "        \n",
    "        df['HSFGM'][i] = total_dict[df[\"HSTeamID\"][i]][df[\"Season\"][i]]['FGM']\n",
    "        df['HSFGA'][i] = total_dict[df[\"HSTeamID\"][i]][df[\"Season\"][i]]['FGA']\n",
    "        df['HSFGM3'][i] = total_dict[df[\"HSTeamID\"][i]][df[\"Season\"][i]]['FGM3']\n",
    "        df['HSFGA3'][i] = total_dict[df[\"HSTeamID\"][i]][df[\"Season\"][i]]['FGA3']\n",
    "        \n",
    "        df['HSFTM'][i] = total_dict[df[\"HSTeamID\"][i]][df[\"Season\"][i]]['FTM']\n",
    "        df['HSFTA'][i] = total_dict[df[\"HSTeamID\"][i]][df[\"Season\"][i]]['FTA']\n",
    "        df['HSOR'][i] = total_dict[df[\"HSTeamID\"][i]][df[\"Season\"][i]]['OR']\n",
    "        df['HSTO'][i] = total_dict[df[\"HSTeamID\"][i]][df[\"Season\"][i]]['TO']\n",
    "        \n",
    "        df['HSDR'][i] = total_dict[df[\"HSTeamID\"][i]][df[\"Season\"][i]]['DR']\n",
    "        df['HSAst'][i] = total_dict[df[\"HSTeamID\"][i]][df[\"Season\"][i]]['Ast']\n",
    "        \n",
    "        df['HSStl'][i] = total_dict[df[\"HSTeamID\"][i]][df[\"Season\"][i]]['Stl']\n",
    "        df['HSBlk'][i] = total_dict[df[\"HSTeamID\"][i]][df[\"Season\"][i]]['Blk']\n",
    "        df['HSPF'][i] = total_dict[df[\"HSTeamID\"][i]][df[\"Season\"][i]]['PF']\n",
    "        \n",
    "        df['HS_avg_def_eff'][i] = total_dict[df[\"HSTeamID\"][i]][df[\"Season\"][i]]['def_eff']\n",
    "        df['HS_wins'][i] = total_dict[df[\"HSTeamID\"][i]][df[\"Season\"][i]]['num_wins']\n",
    "        df['HS_loss'][i] = total_dict[df[\"HSTeamID\"][i]][df[\"Season\"][i]]['num_loss']\n",
    "        \n",
    "        \n",
    "        df['LS_avg_score'][i] = total_dict[df[\"LSTeamID\"][i]][df[\"Season\"][i]]['Score']\n",
    "        df['LS_avg_against'][i] = total_dict[df[\"LSTeamID\"][i]][df[\"Season\"][i]]['Op_score']\n",
    "        \n",
    "        df['LSFGM'][i] = total_dict[df[\"LSTeamID\"][i]][df[\"Season\"][i]]['FGM']\n",
    "        df['LSFGA'][i] = total_dict[df[\"LSTeamID\"][i]][df[\"Season\"][i]]['FGA']\n",
    "        df['LSFGM3'][i] = total_dict[df[\"LSTeamID\"][i]][df[\"Season\"][i]]['FGM3']\n",
    "        df['LSFGA3'][i] = total_dict[df[\"LSTeamID\"][i]][df[\"Season\"][i]]['FGA3']\n",
    "        \n",
    "        df['LSFTM'][i] = total_dict[df[\"LSTeamID\"][i]][df[\"Season\"][i]]['FTM']\n",
    "        df['LSFTA'][i] = total_dict[df[\"LSTeamID\"][i]][df[\"Season\"][i]]['FTA']\n",
    "        df['LSOR'][i] = total_dict[df[\"LSTeamID\"][i]][df[\"Season\"][i]]['OR']\n",
    "        df['LSTO'][i] = total_dict[df[\"LSTeamID\"][i]][df[\"Season\"][i]]['TO']\n",
    "        \n",
    "        df['LSDR'][i] = total_dict[df[\"LSTeamID\"][i]][df[\"Season\"][i]]['DR']\n",
    "        df['LSAst'][i] = total_dict[df[\"LSTeamID\"][i]][df[\"Season\"][i]]['Ast']\n",
    "        \n",
    "        df['LSStl'][i] = total_dict[df[\"LSTeamID\"][i]][df[\"Season\"][i]]['Stl']\n",
    "        df['LSBlk'][i] = total_dict[df[\"LSTeamID\"][i]][df[\"Season\"][i]]['Blk']\n",
    "        df['LSPF'][i] = total_dict[df[\"LSTeamID\"][i]][df[\"Season\"][i]]['PF']\n",
    "        \n",
    "        df['LS_avg_def_eff'][i] = total_dict[df[\"LSTeamID\"][i]][df[\"Season\"][i]]['def_eff']\n",
    "        df['LS_wins'][i] = total_dict[df[\"LSTeamID\"][i]][df[\"Season\"][i]]['num_wins']\n",
    "        df['LS_loss'][i] = total_dict[df[\"LSTeamID\"][i]][df[\"Season\"][i]]['num_loss']  \n",
    "    \n",
    "    #drop columns we dont need \n",
    "    df = df.drop(columns = ['DayNum', 'WLoc', 'NumOT'])   \n",
    "    return df      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddcdd281-aba3-46c6-ab4d-6b34df3a806b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tour5 = get_reg_avgs(tour4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ba841b-0a47-41f6-8295-0dd1d56f431c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d48f4dc-1f00-4a1e-8ace-ee62031b950b",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tour5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cbcab35-9a45-421d-b29e-dd692c677764",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_conf_data(df):\n",
    "    '''\n",
    "    Parameters: takes in data frame from previous function with features almost \n",
    "    ready for trainign, \n",
    "    \n",
    "    uses conf_champ dictionary and conf_affil dictionaries to get conference afiliation \n",
    "    and if the teams in question were conference champions. \n",
    "    \n",
    "    NOTE: the ivy league only started playing a conference tournament in 2017\n",
    "    so if not found insert ivy lol \n",
    "    \n",
    "    \n",
    "    '''\n",
    "    #https://torchcollegerecruiting.com/z_a3_high_mid_low_majors/1641907452726x394612841469246460\n",
    "    # high vs medium vs low major conferences \n",
    "\n",
    "    power_conf = [\"acc\", 'sec', \"big_ten\", \n",
    "                  \"big_twelve\", \"pac_ten\", \"pac_twelve\"]\n",
    "\n",
    "    mid_major = ['cusa', 'aac', 'mwc', 'sun_belt', 'ivy', \n",
    "                 'mac', 'big_sky', 'meac' ,'southland', \n",
    "                 'summit', 'wac', 'wcc',]\n",
    "\n",
    "    low_major = ['aec', 'a_ten', 'big_south', 'caa', \n",
    "                 'nec', 'patriot', 'southern', 'swac', \n",
    "                 'mvc', 'a_sun', 'ovc', 'horizon', \n",
    "                 'maac', 'swac']    \n",
    "    \n",
    "    \n",
    "    hs_conf= []\n",
    "    ls_conf = []\n",
    "    \n",
    "    hs_conf_champ = []\n",
    "    ls_conf_champ = []\n",
    "    for i in range(len(df)):\n",
    "        try:\n",
    "            hs_conf.append(conf_affil[df['Season'][i]][df['HSTeamID'][i]])\n",
    "\n",
    "        except:\n",
    "            hs_conf.append('ivy')\n",
    "\n",
    "        try:\n",
    "            ls_conf.append(conf_affil[df['Season'][i]][df['LSTeamID'][i]]) \n",
    "        except:\n",
    "            ls_conf.append('ivy')\n",
    "        \n",
    "        \n",
    "        if df['HSTeamID'][i] in list(conf_champs[df['Season'][i]].keys()):\n",
    "            hs_conf_champ.append(1)\n",
    "        else:\n",
    "            hs_conf_champ.append(0)\n",
    "        \n",
    "        if df['LSTeamID'][i] in list(conf_champs[df['Season'][i]].keys()):\n",
    "            ls_conf_champ.append(1)\n",
    "        else:\n",
    "            ls_conf_champ.append(0)\n",
    "        \n",
    "        \n",
    "    \n",
    "    df['HS_Conf'] = hs_conf\n",
    "    df['LS_Conf'] = ls_conf\n",
    "    \n",
    "    df['HS_power_conf']= df['HS_Conf'].apply(lambda x: 1 if x in power_conf else 0)\n",
    "    df['HS_mid_conf']= df['HS_Conf'].apply(lambda x: 1 if x in mid_major else 0)\n",
    "    df['HS_low_conf'] = df['HS_Conf'].apply(lambda x: 1 if x in low_major else 0)\n",
    "    \n",
    "    df['LS_power_conf']= df['LS_Conf'].apply(lambda x: 1 if x in power_conf else 0)\n",
    "    df['LS_mid_conf']= df['LS_Conf'].apply(lambda x: 1 if x in mid_major else 0)\n",
    "    df['LS_low_conf'] = df['LS_Conf'].apply(lambda x: 1 if x in low_major else 0)    \n",
    "\n",
    "    df['HS_conf_champ'] = hs_conf_champ\n",
    "    df['LS_conf_champ'] = ls_conf_champ\n",
    "    \n",
    "    \n",
    "    df = df.drop(columns = ['HS_Conf', 'LS_Conf'])\n",
    "    return df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf7ee36-d394-4e91-92cd-eab2ff1d81ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "tour6 = get_conf_data(tour5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ece569-23dd-4e47-a211-f9446603dad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tour6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055d62f9-338c-4c6e-9a9c-1c4bbb775616",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9acc4bd1-b6e4-4f89-907f-1a5c963e6071",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad91a45-412c-450e-8e36-bb553bfbae42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_page_rank(df):\n",
    "    '''\n",
    "    Parameters: tournament DF in the preperation phase for modeling, \n",
    "    ADDITIONALLY: page_rank dictionary\n",
    "    \n",
    "    Having completed the page rank steps above, we have a page rank dictionary\n",
    "    \"page_rank\", which has the by season by team values for the different teams, \n",
    "    we will use this to populate page rank for low and high seeds \n",
    "    \n",
    "    '''\n",
    "    df['HS_page_rank'] = np.zeros(len(df))\n",
    "    df['LS_page_rank'] = np.zeros(len(df))\n",
    "    \n",
    "    for i in tqdm(range(len(df))):\n",
    "        #print(i)\n",
    "        df['HS_page_rank'][i] = page_rank[df['Season'][i]][str(df['HSTeamID'][i])]\n",
    "        df['LS_page_rank'][i] = page_rank[df['Season'][i]][str(df['LSTeamID'][i])]\n",
    "    return df     \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d01365d-6382-488d-b410-884474d562eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3fb05a5-cc48-42e3-aba4-1ead624ccc4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tour7 = apply_page_rank(tour6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2990843d-9662-4e9c-8045-4fc979cb224e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hist_wins(df):\n",
    "    '''\n",
    "    Parameters: tournament DF in the preperation phase for modeling, \n",
    "    ADDITIONALLY: tw (tournament wins) dictionary\n",
    "    \n",
    "    having discussed this above, we probably cannot use teamID's as \n",
    "    features simply because those will be constantly changing throughout the rounds \n",
    "    and years of the tournament, makign it difficult to have consistent features if \n",
    "    we trim down / out teams that have been elemenated, but also the dimmensionality \n",
    "    will be enormous and be very sparce, so instead we will use historical tournament \n",
    "    winning percentage to try and capture some of that \"recognize me I've won in the \n",
    "    tournament a lot\" vibe we might be able to get from team_id \n",
    "    \n",
    "    '''\n",
    "    df['HS_historical_tournament_win%'] = np.zeros(len(df))\n",
    "    df['LS_historical_tournament_win%'] = np.zeros(len(df))\n",
    "    \n",
    "    \n",
    "    for i in tqdm(range(len(df))):\n",
    "        try:\n",
    "            df['HS_historical_tournament_win%'][i] = tw[df['Season'][i]][df['HSTeamID'][i]]\n",
    "            df['LS_historical_tournament_win%'][i] = tw[df['Season'][i]][df['LSTeamID'][i]]\n",
    "        except:\n",
    "            df['LS_historical_tournament_win%'][i] = 0\n",
    "        \n",
    "    return df \n",
    "                                                                \n",
    "                                                    \n",
    "                                                                  \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc5c823-0292-426d-8eaa-0c3a32873749",
   "metadata": {},
   "outputs": [],
   "source": [
    "tour8 = get_hist_wins(tour7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80be039-23b7-4636-8381-450bbc2e3705",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_target(df):\n",
    "    '''\n",
    "    Parameters: DF in prep stage for modeling\n",
    "    \n",
    "    with the feature building steps finally complete, \n",
    "    our last feature is for us to create our target variabe \n",
    "    did the higher seed win or lose? \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    df['target'] = tour4.apply(lambda x: 1 if x['HS_Score'] > x['LS_Score'] else 0, axis = 1)\n",
    "    return df \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381cb0c1-bc83-4023-9796-ea93d1226361",
   "metadata": {},
   "outputs": [],
   "source": [
    "tour9 = get_target(tour8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0012a4f-3879-4377-978b-ffbedeb3575b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tour9['target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de178209-9c26-400b-877f-756d69786e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "tour9.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cdde3eb-074f-4aa3-9c09-38b05023f618",
   "metadata": {},
   "outputs": [],
   "source": [
    "# high_corr = tour9[['LPSeed', 'LTeamSeed', 'W_NumWins', \"W_avg_pts\",\n",
    "#                           'L_Numlosses', 'WP6', 'W_Avg_DR', 'W_FG%', 'W3%', \"W_Avg_OR\", \"L_ops_avg_pts\"]]\n",
    "corr_mat = tour9.corr()\n",
    "f, ax = plt.subplots(figsize = (10, 6))\n",
    "sns.heatmap(corr_mat,vmax=.8,square=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24935a7e-a75c-4eb9-adf9-1cdcc711de4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e8dc19-13ff-4dce-94d9-862b0e9e629d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
